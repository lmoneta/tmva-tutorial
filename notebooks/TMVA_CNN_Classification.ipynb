{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://oproject.org/tiki-download_file.php?fileId=8&display&x=450&y=128\">\n",
    "<img src=\"http://files.oproject.org/tmvalogo.png\" height=\"50%\" width=\"50%\">\n",
    "\n",
    "# TMVA Classification Example Using a Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare Factory\n",
    "\n",
    "Create the Factory class. Later you can choose the methods\n",
    "whose performance you'd like to investigate. \n",
    "\n",
    "The factory is the major TMVA object you have to interact with. Here is the list of parameters you need to pass\n",
    "\n",
    " - The first argument is the base of the name of all the output\n",
    "weightfiles in the directory weight/ that will be created with the \n",
    "method parameters \n",
    "\n",
    " - The second argument is the output file for the training results\n",
    "  \n",
    " - The third argument is a string option defining some general configuration for the TMVA session. For example all TMVA output can be suppressed by removing the \"!\" (not) in front of the \"Silent\" argument in the option string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "TMVA::Tools::Instance();\n",
    "\n",
    " // for using Keras\n",
    "gSystem->Setenv(\"KERAS_BACKEND\",\"tensorflow\"); \n",
    "TMVA::PyMethodBase::PyInitialize();\n",
    "\n",
    "\n",
    "\n",
    "auto outputFile = TFile::Open(\"CNN_ClassificationOutput.root\", \"RECREATE\");\n",
    "\n",
    "TMVA::Factory factory(\"TMVA_CNN_Classification\", outputFile,\n",
    "                      \"!V:ROC:!Silent:Color:!DrawProgressBar:AnalysisType=Classification\" ); \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare DataLoader(s)\n",
    "\n",
    "The next step is to declare the DataLoader class that deals with input variables \n",
    "\n",
    "Define the input variables that shall be used for the MVA training\n",
    "note that you may also use variable expressions, which can be parsed by TTree::Draw( \"expression\" )]\n",
    "\n",
    "In this case the input data consists of an image of 16x16 pixels. Each single pixel is a branch in a ROOT TTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TMVA::DataLoader * loader = new TMVA::DataLoader(\"dataset\");\n",
    "\n",
    "int imgSize = 8 * 8; \n",
    "for(auto i = 0; i < imgSize; i++)\n",
    " {\n",
    "     loader->AddVariable(Form(\"var%d\",i),'F');\n",
    " }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Dataset(s)\n",
    "\n",
    "Define input data file and signal and background trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataSetInfo              : [dataset] : Added class \"Signal\"\n",
      "                         : Add Tree sig_tree of type Signal with 10000 events\n",
      "DataSetInfo              : [dataset] : Added class \"Background\"\n",
      "                         : Add Tree bkg_tree of type Background with 10000 events\n"
     ]
    }
   ],
   "source": [
    "TString inputFileName = \"images_data.root\";\n",
    "\n",
    "//TString inputFileName = \"tmva_class_example.root\";\n",
    "\n",
    "auto inputFile = TFile::Open( inputFileName );\n",
    "\n",
    "// --- Register the training and test trees\n",
    "\n",
    "TTree *signalTree     = (TTree*)inputFile->Get(\"sig_tree\");\n",
    "TTree *backgroundTree = (TTree*)inputFile->Get(\"bkg_tree\");\n",
    "\n",
    "// global event weights per tree (see below for setting event-wise weights)\n",
    "Double_t signalWeight     = 1.0;\n",
    "Double_t backgroundWeight = 1.0;\n",
    "   \n",
    "// You can add an arbitrary number of signal or background trees\n",
    "loader->AddSignalTree    ( signalTree,     signalWeight     );\n",
    "loader->AddBackgroundTree( backgroundTree, backgroundWeight );\n",
    "\n",
    "\n",
    "// Set individual event weights (the variables must exist in the original TTree)\n",
    "//    for signal    : factory->SetSignalWeightExpression    (\"weight1*weight2\");\n",
    "//    for background: factory->SetBackgroundWeightExpression(\"weight1*weight2\");\n",
    "//loader->SetBackgroundWeightExpression( \"weight\" );\n",
    "\n",
    "// Apply additional cuts on the signal and background samples (can be different)\n",
    "TCut mycuts = \"\"; // for example: TCut mycuts = \"abs(var1)<0.5 && abs(var2-0.5)<1\";\n",
    "TCut mycutb = \"\"; // for example: TCut mycutb = \"abs(var1)<0.5\";\n",
    "\n",
    "// Tell the factory how to use the training and testing events\n",
    "//\n",
    "// If no numbers of events are given, half of the events in the tree are used \n",
    "// for training, and the other half for testing:\n",
    "//    loader->PrepareTrainingAndTestTree( mycut, \"SplitMode=random:!V\" );\n",
    "// To also specify the number of testing events, use:\n",
    "\n",
    "loader->PrepareTrainingAndTestTree( mycuts, mycutb,\n",
    "                                    \"nTrain_Signal=0:nTrain_Background=0:SplitMode=Random:NormMode=NumEvents:!V\" );\n",
    "\n",
    "\n",
    "\n",
    "//loader->PrepareTrainingAndTestTree(mycuts, mycutb,\n",
    "//                                   \"nTrain_Signal=5000:nTrain_Background=5000:nTest_Signal=5000:nTest_Background=5000:SplitMode=Random:NormMode=NumEvents:!V\" ); \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Booking Methods\n",
    "\n",
    "Here we book the TMVA methods. We book a Likelihood based on KDE, a Fischer discriminant and a BDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factory                  : Booking method: \u001b[1mBDT\u001b[0m\n",
      "                         : \n",
      "DataSetFactory           : [dataset] : Number of events in input trees\n",
      "                         : \n",
      "                         : \n",
      "                         : Number of training and testing events\n",
      "                         : ---------------------------------------------------------------------------\n",
      "                         : Signal     -- training events            : 5000\n",
      "                         : Signal     -- testing events             : 5000\n",
      "                         : Signal     -- training and testing events: 10000\n",
      "                         : Background -- training events            : 5000\n",
      "                         : Background -- testing events             : 5000\n",
      "                         : Background -- training and testing events: 10000\n",
      "                         : \n",
      "DataSetInfo              : Correlation matrix (Signal):\n",
      "                         : ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "                         :             var0    var1    var2    var3    var4    var5    var6    var7    var8    var9   var10   var11   var12   var13   var14   var15   var16   var17   var18   var19   var20   var21   var22   var23   var24   var25   var26   var27   var28   var29   var30   var31   var32   var33   var34   var35   var36   var37   var38   var39   var40   var41   var42   var43   var44   var45   var46   var47   var48   var49   var50   var51   var52   var53   var54   var55   var56   var57   var58   var59   var60   var61   var62   var63\n",
      "                         :    var0:  +1.000  +0.270  +0.261  +0.210  +0.091  -0.039  -0.087  -0.103  +0.279  +0.321  +0.296  +0.202  +0.085  -0.098  -0.151  -0.145  +0.308  +0.301  +0.313  +0.190  +0.010  -0.095  -0.191  -0.184  +0.287  +0.276  +0.252  +0.113  -0.070  -0.217  -0.250  -0.221  +0.227  +0.233  +0.188  +0.026  -0.167  -0.252  -0.301  -0.233  +0.177  +0.155  +0.078  -0.048  -0.203  -0.273  -0.293  -0.241  +0.110  +0.075  +0.018  -0.103  -0.219  -0.261  -0.255  -0.226  +0.059  +0.014  -0.029  -0.113  -0.197  -0.236  -0.220  -0.184\n",
      "                         :    var1:  +0.270  +1.000  +0.360  +0.290  +0.176  +0.029  -0.055  -0.073  +0.345  +0.395  +0.380  +0.288  +0.164  -0.016  -0.082  -0.147  +0.337  +0.370  +0.370  +0.274  +0.074  -0.097  -0.178  -0.174  +0.302  +0.325  +0.294  +0.182  -0.076  -0.204  -0.269  -0.239  +0.241  +0.229  +0.188  +0.023  -0.196  -0.292  -0.316  -0.270  +0.154  +0.141  +0.050  -0.135  -0.282  -0.348  -0.323  -0.294  +0.065  +0.037  -0.056  -0.181  -0.303  -0.325  -0.327  -0.266  +0.036  +0.000  -0.084  -0.205  -0.269  -0.302  -0.290  -0.233\n",
      "                         :    var2:  +0.261  +0.360  +1.000  +0.348  +0.251  +0.147  +0.027  +0.004  +0.303  +0.379  +0.395  +0.352  +0.248  +0.072  -0.010  -0.069  +0.279  +0.335  +0.375  +0.293  +0.154  +0.006  -0.074  -0.109  +0.252  +0.262  +0.228  +0.184  +0.006  -0.129  -0.193  -0.186  +0.152  +0.158  +0.122  -0.026  -0.199  -0.271  -0.259  -0.237  +0.070  +0.037  -0.033  -0.190  -0.284  -0.335  -0.297  -0.267  -0.002  -0.043  -0.136  -0.232  -0.344  -0.364  -0.308  -0.262  -0.054  -0.080  -0.163  -0.270  -0.307  -0.353  -0.303  -0.206\n",
      "                         :    var3:  +0.210  +0.290  +0.348  +1.000  +0.336  +0.248  +0.160  +0.080  +0.217  +0.299  +0.342  +0.364  +0.332  +0.211  +0.114  +0.058  +0.167  +0.216  +0.269  +0.298  +0.228  +0.147  +0.057  +0.003  +0.117  +0.136  +0.148  +0.122  +0.071  -0.001  -0.046  -0.053  +0.025  +0.014  -0.035  -0.076  -0.134  -0.153  -0.145  -0.124  -0.030  -0.103  -0.160  -0.238  -0.293  -0.259  -0.203  -0.187  -0.108  -0.170  -0.253  -0.329  -0.361  -0.323  -0.263  -0.224  -0.129  -0.213  -0.274  -0.333  -0.342  -0.308  -0.282  -0.186\n",
      "                         :    var4:  +0.091  +0.176  +0.251  +0.336  +1.000  +0.333  +0.279  +0.189  +0.088  +0.151  +0.215  +0.319  +0.362  +0.333  +0.283  +0.187  +0.033  +0.060  +0.131  +0.258  +0.270  +0.278  +0.207  +0.173  -0.025  -0.038  -0.004  +0.074  +0.123  +0.146  +0.124  +0.090  -0.114  -0.148  -0.161  -0.142  -0.084  -0.039  +0.003  +0.014  -0.165  -0.217  -0.256  -0.283  -0.247  -0.149  -0.093  -0.061  -0.222  -0.253  -0.329  -0.365  -0.315  -0.247  -0.155  -0.111  -0.194  -0.263  -0.321  -0.346  -0.331  -0.275  -0.179  -0.113\n",
      "                         :    var5:  -0.039  +0.029  +0.147  +0.248  +0.333  +1.000  +0.326  +0.283  -0.036  -0.006  +0.087  +0.242  +0.355  +0.386  +0.372  +0.277  -0.102  -0.085  +0.007  +0.162  +0.278  +0.356  +0.311  +0.284  -0.159  -0.172  -0.143  +0.001  +0.161  +0.258  +0.257  +0.218  -0.225  -0.268  -0.261  -0.180  +0.011  +0.109  +0.155  +0.153  -0.260  -0.309  -0.331  -0.273  -0.177  -0.027  +0.062  +0.079  -0.247  -0.316  -0.356  -0.323  -0.248  -0.122  -0.032  -0.011  -0.213  -0.295  -0.321  -0.317  -0.261  -0.162  -0.101  -0.042\n",
      "                         :    var6:  -0.087  -0.055  +0.027  +0.160  +0.279  +0.326  +1.000  +0.280  -0.126  -0.101  -0.015  +0.145  +0.285  +0.384  +0.377  +0.333  -0.170  -0.164  -0.098  +0.073  +0.255  +0.344  +0.372  +0.342  -0.233  -0.253  -0.204  -0.063  +0.151  +0.293  +0.315  +0.284  -0.281  -0.289  -0.304  -0.190  +0.026  +0.145  +0.230  +0.222  -0.268  -0.331  -0.342  -0.263  -0.102  +0.056  +0.152  +0.131  -0.274  -0.318  -0.331  -0.275  -0.140  -0.033  +0.065  +0.082  -0.211  -0.268  -0.308  -0.274  -0.174  -0.090  -0.001  +0.009\n",
      "                         :    var7:  -0.103  -0.073  +0.004  +0.080  +0.189  +0.283  +0.280  +1.000  -0.126  -0.118  -0.062  +0.085  +0.198  +0.295  +0.331  +0.264  -0.181  -0.158  -0.103  +0.021  +0.175  +0.298  +0.311  +0.277  -0.203  -0.222  -0.203  -0.092  +0.124  +0.256  +0.291  +0.252  -0.224  -0.284  -0.261  -0.158  +0.046  +0.166  +0.216  +0.219  -0.244  -0.269  -0.278  -0.208  -0.062  +0.081  +0.158  +0.153  -0.211  -0.265  -0.250  -0.207  -0.094  +0.003  +0.067  +0.095  -0.194  -0.202  -0.214  -0.198  -0.127  -0.034  +0.033  +0.036\n",
      "                         :    var8:  +0.279  +0.345  +0.303  +0.217  +0.088  -0.036  -0.126  -0.126  +1.000  +0.369  +0.340  +0.222  +0.051  -0.128  -0.185  -0.183  +0.364  +0.377  +0.357  +0.213  -0.020  -0.172  -0.240  -0.252  +0.342  +0.371  +0.321  +0.158  -0.105  -0.271  -0.314  -0.280  +0.283  +0.305  +0.227  +0.064  -0.190  -0.323  -0.355  -0.297  +0.227  +0.215  +0.139  -0.058  -0.217  -0.345  -0.351  -0.296  +0.175  +0.132  +0.044  -0.099  -0.258  -0.296  -0.320  -0.257  +0.099  +0.062  -0.001  -0.088  -0.221  -0.277  -0.278  -0.185\n",
      "                         :    var9:  +0.321  +0.395  +0.379  +0.299  +0.151  -0.006  -0.101  -0.118  +0.369  +1.000  +0.428  +0.311  +0.119  -0.077  -0.167  -0.181  +0.385  +0.432  +0.418  +0.271  +0.035  -0.159  -0.238  -0.245  +0.370  +0.385  +0.346  +0.197  -0.096  -0.261  -0.337  -0.306  +0.297  +0.329  +0.248  +0.055  -0.210  -0.354  -0.384  -0.347  +0.226  +0.221  +0.113  -0.079  -0.304  -0.397  -0.396  -0.354  +0.140  +0.092  +0.014  -0.149  -0.332  -0.379  -0.385  -0.313  +0.074  +0.041  -0.051  -0.183  -0.290  -0.345  -0.354  -0.264\n",
      "                         :   var10:  +0.296  +0.380  +0.395  +0.342  +0.215  +0.087  -0.015  -0.062  +0.340  +0.428  +1.000  +0.360  +0.212  +0.037  -0.061  -0.124  +0.325  +0.399  +0.406  +0.309  +0.112  -0.064  -0.172  -0.185  +0.311  +0.340  +0.313  +0.201  -0.027  -0.195  -0.261  -0.260  +0.221  +0.241  +0.169  +0.019  -0.199  -0.324  -0.340  -0.310  +0.158  +0.128  +0.016  -0.141  -0.327  -0.397  -0.376  -0.340  +0.069  +0.025  -0.065  -0.212  -0.344  -0.396  -0.388  -0.317  +0.002  -0.055  -0.125  -0.253  -0.326  -0.365  -0.345  -0.288\n",
      "                         :   var11:  +0.202  +0.288  +0.352  +0.364  +0.319  +0.242  +0.145  +0.085  +0.222  +0.311  +0.360  +1.000  +0.329  +0.186  +0.115  +0.036  +0.166  +0.234  +0.295  +0.299  +0.209  +0.117  +0.034  -0.003  +0.129  +0.148  +0.156  +0.150  +0.041  -0.017  -0.085  -0.095  +0.043  +0.052  +0.010  -0.050  -0.140  -0.193  -0.181  -0.167  +0.003  -0.050  -0.124  -0.222  -0.308  -0.282  -0.235  -0.223  -0.077  -0.140  -0.214  -0.312  -0.348  -0.340  -0.290  -0.230  -0.105  -0.172  -0.229  -0.315  -0.349  -0.343  -0.293  -0.217\n",
      "                         :   var12:  +0.085  +0.164  +0.248  +0.332  +0.362  +0.355  +0.285  +0.198  +0.051  +0.119  +0.212  +0.329  +1.000  +0.347  +0.295  +0.195  -0.009  +0.041  +0.133  +0.257  +0.311  +0.281  +0.237  +0.171  -0.057  -0.048  -0.023  +0.087  +0.142  +0.158  +0.141  +0.122  -0.147  -0.178  -0.180  -0.169  -0.056  -0.006  +0.034  +0.043  -0.190  -0.246  -0.293  -0.298  -0.254  -0.128  -0.054  -0.029  -0.237  -0.292  -0.331  -0.342  -0.295  -0.218  -0.131  -0.096  -0.239  -0.289  -0.338  -0.371  -0.313  -0.272  -0.188  -0.106\n",
      "                         :   var13:  -0.098  -0.016  +0.072  +0.211  +0.333  +0.386  +0.384  +0.295  -0.128  -0.077  +0.037  +0.186  +0.347  +1.000  +0.414  +0.324  -0.195  -0.167  -0.070  +0.135  +0.285  +0.402  +0.377  +0.347  -0.246  -0.239  -0.212  -0.042  +0.184  +0.320  +0.338  +0.282  -0.294  -0.340  -0.337  -0.202  +0.013  +0.175  +0.224  +0.219  -0.305  -0.367  -0.372  -0.304  -0.116  +0.042  +0.142  +0.154  -0.336  -0.362  -0.384  -0.325  -0.188  -0.065  +0.026  +0.051  -0.269  -0.326  -0.347  -0.326  -0.236  -0.124  -0.028  -0.019\n",
      "                         :   var14:  -0.151  -0.082  -0.010  +0.114  +0.283  +0.372  +0.377  +0.331  -0.185  -0.167  -0.061  +0.115  +0.295  +0.414  +1.000  +0.353  -0.241  -0.249  -0.143  +0.051  +0.268  +0.404  +0.418  +0.371  -0.299  -0.307  -0.276  -0.077  +0.185  +0.344  +0.390  +0.342  -0.332  -0.369  -0.353  -0.201  +0.073  +0.236  +0.290  +0.283  -0.324  -0.392  -0.377  -0.270  -0.062  +0.107  +0.212  +0.206  -0.308  -0.378  -0.361  -0.284  -0.115  +0.005  +0.126  +0.120  -0.253  -0.304  -0.320  -0.277  -0.164  -0.044  +0.044  +0.047\n",
      "                         :   var15:  -0.145  -0.147  -0.069  +0.058  +0.187  +0.277  +0.333  +0.264  -0.183  -0.181  -0.124  +0.036  +0.195  +0.324  +0.353  +1.000  -0.246  -0.256  -0.196  -0.029  +0.191  +0.349  +0.378  +0.367  -0.274  -0.311  -0.280  -0.128  +0.149  +0.319  +0.378  +0.345  -0.301  -0.349  -0.318  -0.185  +0.060  +0.256  +0.309  +0.297  -0.303  -0.343  -0.330  -0.221  -0.037  +0.163  +0.240  +0.248  -0.260  -0.312  -0.319  -0.217  -0.056  +0.053  +0.153  +0.164  -0.218  -0.249  -0.252  -0.182  -0.103  +0.026  +0.101  +0.103\n",
      "                         :   var16:  +0.308  +0.337  +0.279  +0.167  +0.033  -0.102  -0.170  -0.181  +0.364  +0.385  +0.325  +0.166  -0.009  -0.195  -0.241  -0.246  +1.000  +0.407  +0.381  +0.197  -0.064  -0.258  -0.323  -0.294  +0.389  +0.399  +0.350  +0.170  -0.131  -0.319  -0.374  -0.326  +0.364  +0.379  +0.292  +0.105  -0.193  -0.365  -0.408  -0.362  +0.306  +0.303  +0.207  +0.012  -0.211  -0.360  -0.377  -0.339  +0.233  +0.221  +0.111  -0.027  -0.222  -0.322  -0.340  -0.316  +0.165  +0.136  +0.048  -0.055  -0.188  -0.263  -0.289  -0.224\n",
      "                         :   var17:  +0.301  +0.370  +0.335  +0.216  +0.060  -0.085  -0.164  -0.158  +0.377  +0.432  +0.399  +0.234  +0.041  -0.167  -0.249  -0.256  +0.407  +1.000  +0.416  +0.217  -0.041  -0.254  -0.335  -0.325  +0.426  +0.440  +0.384  +0.200  -0.141  -0.327  -0.412  -0.361  +0.372  +0.413  +0.304  +0.090  -0.236  -0.391  -0.430  -0.402  +0.306  +0.301  +0.201  -0.012  -0.252  -0.401  -0.417  -0.405  +0.234  +0.222  +0.130  -0.064  -0.265  -0.361  -0.406  -0.337  +0.162  +0.104  +0.045  -0.077  -0.213  -0.333  -0.331  -0.281\n",
      "                         :   var18:  +0.313  +0.370  +0.375  +0.269  +0.131  +0.007  -0.098  -0.103  +0.357  +0.418  +0.406  +0.295  +0.133  -0.070  -0.143  -0.196  +0.381  +0.416  +1.000  +0.264  +0.060  -0.152  -0.242  -0.251  +0.378  +0.368  +0.358  +0.198  -0.058  -0.255  -0.337  -0.300  +0.297  +0.310  +0.258  +0.051  -0.201  -0.345  -0.384  -0.336  +0.227  +0.217  +0.113  -0.050  -0.293  -0.411  -0.393  -0.365  +0.135  +0.117  +0.013  -0.133  -0.323  -0.380  -0.386  -0.336  +0.074  +0.021  -0.033  -0.155  -0.286  -0.364  -0.347  -0.255\n",
      "                         :   var19:  +0.190  +0.274  +0.293  +0.298  +0.258  +0.162  +0.073  +0.021  +0.213  +0.271  +0.309  +0.299  +0.257  +0.135  +0.051  -0.029  +0.197  +0.217  +0.264  +1.000  +0.170  +0.062  -0.019  -0.078  +0.153  +0.173  +0.175  +0.132  +0.038  -0.054  -0.123  -0.121  +0.093  +0.083  +0.071  -0.020  -0.124  -0.191  -0.211  -0.170  +0.027  -0.007  -0.043  -0.168  -0.252  -0.286  -0.249  -0.228  -0.032  -0.064  -0.138  -0.226  -0.289  -0.304  -0.276  -0.240  -0.073  -0.106  -0.163  -0.261  -0.313  -0.308  -0.299  -0.209\n",
      "                         :   var20:  +0.010  +0.074  +0.154  +0.228  +0.270  +0.278  +0.255  +0.175  -0.020  +0.035  +0.112  +0.209  +0.311  +0.285  +0.268  +0.191  -0.064  -0.041  +0.060  +0.170  +1.000  +0.270  +0.220  +0.198  -0.130  -0.116  -0.071  +0.012  +0.125  +0.193  +0.168  +0.136  -0.172  -0.188  -0.191  -0.114  -0.016  +0.063  +0.079  +0.090  -0.215  -0.253  -0.255  -0.222  -0.138  -0.069  +0.018  +0.032  -0.225  -0.272  -0.309  -0.282  -0.222  -0.145  -0.067  -0.025  -0.205  -0.262  -0.268  -0.284  -0.227  -0.156  -0.108  -0.072\n",
      "                         :   var21:  -0.095  -0.097  +0.006  +0.147  +0.278  +0.356  +0.344  +0.298  -0.172  -0.159  -0.064  +0.117  +0.281  +0.402  +0.404  +0.349  -0.258  -0.254  -0.152  +0.062  +0.270  +1.000  +0.418  +0.372  -0.309  -0.321  -0.277  -0.090  +0.212  +0.362  +0.383  +0.329  -0.345  -0.388  -0.359  -0.197  +0.065  +0.246  +0.293  +0.296  -0.365  -0.411  -0.379  -0.263  -0.057  +0.136  +0.212  +0.220  -0.336  -0.386  -0.394  -0.287  -0.113  +0.022  +0.115  +0.160  -0.283  -0.338  -0.324  -0.260  -0.154  -0.015  +0.061  +0.069\n",
      "                         :   var22:  -0.191  -0.178  -0.074  +0.057  +0.207  +0.311  +0.372  +0.311  -0.240  -0.238  -0.172  +0.034  +0.237  +0.377  +0.418  +0.378  -0.323  -0.335  -0.242  -0.019  +0.220  +0.418  +1.000  +0.410  -0.357  -0.386  -0.347  -0.141  +0.184  +0.396  +0.449  +0.407  -0.397  -0.439  -0.410  -0.239  +0.118  +0.298  +0.385  +0.379  -0.378  -0.427  -0.395  -0.238  +0.017  +0.228  +0.308  +0.301  -0.338  -0.377  -0.370  -0.243  -0.039  +0.122  +0.233  +0.210  -0.288  -0.316  -0.302  -0.239  -0.097  +0.042  +0.142  +0.141\n",
      "                         :   var23:  -0.184  -0.174  -0.109  +0.003  +0.173  +0.284  +0.342  +0.277  -0.252  -0.245  -0.185  -0.003  +0.171  +0.347  +0.371  +0.367  -0.294  -0.325  -0.251  -0.078  +0.198  +0.372  +0.410  +1.000  -0.352  -0.367  -0.337  -0.132  +0.170  +0.338  +0.435  +0.389  -0.371  -0.410  -0.376  -0.208  +0.095  +0.299  +0.371  +0.357  -0.330  -0.393  -0.355  -0.220  +0.030  +0.239  +0.307  +0.312  -0.322  -0.343  -0.310  -0.207  -0.018  +0.131  +0.247  +0.241  -0.249  -0.289  -0.269  -0.177  -0.050  +0.089  +0.139  +0.154\n",
      "                         :   var24:  +0.287  +0.302  +0.252  +0.117  -0.025  -0.159  -0.233  -0.203  +0.342  +0.370  +0.311  +0.129  -0.057  -0.246  -0.299  -0.274  +0.389  +0.426  +0.378  +0.153  -0.130  -0.309  -0.357  -0.352  +1.000  +0.432  +0.375  +0.155  -0.158  -0.389  -0.431  -0.351  +0.393  +0.436  +0.360  +0.106  -0.212  -0.388  -0.425  -0.392  +0.354  +0.358  +0.275  +0.068  -0.203  -0.361  -0.392  -0.377  +0.278  +0.285  +0.215  +0.024  -0.188  -0.298  -0.348  -0.312  +0.214  +0.210  +0.138  +0.001  -0.132  -0.254  -0.276  -0.253\n",
      "                         :   var25:  +0.276  +0.325  +0.262  +0.136  -0.038  -0.172  -0.253  -0.222  +0.371  +0.385  +0.340  +0.148  -0.048  -0.239  -0.307  -0.311  +0.399  +0.440  +0.368  +0.173  -0.116  -0.321  -0.386  -0.367  +0.432  +1.000  +0.401  +0.178  -0.174  -0.394  -0.460  -0.388  +0.416  +0.447  +0.371  +0.145  -0.207  -0.405  -0.456  -0.422  +0.381  +0.365  +0.281  +0.070  -0.195  -0.398  -0.432  -0.384  +0.308  +0.289  +0.214  +0.046  -0.175  -0.318  -0.385  -0.341  +0.206  +0.207  +0.152  +0.027  -0.163  -0.270  -0.313  -0.257\n",
      "                         :   var26:  +0.252  +0.294  +0.228  +0.148  -0.004  -0.143  -0.204  -0.203  +0.321  +0.346  +0.313  +0.156  -0.023  -0.212  -0.276  -0.280  +0.350  +0.384  +0.358  +0.175  -0.071  -0.277  -0.347  -0.337  +0.375  +0.401  +1.000  +0.164  -0.140  -0.336  -0.408  -0.364  +0.340  +0.388  +0.332  +0.139  -0.186  -0.376  -0.423  -0.386  +0.312  +0.333  +0.246  +0.062  -0.185  -0.363  -0.387  -0.362  +0.239  +0.245  +0.183  +0.026  -0.179  -0.312  -0.353  -0.331  +0.176  +0.168  +0.121  -0.005  -0.153  -0.239  -0.286  -0.260\n",
      "                         :   var27:  +0.113  +0.182  +0.184  +0.122  +0.074  +0.001  -0.063  -0.092  +0.158  +0.197  +0.201  +0.150  +0.087  -0.042  -0.077  -0.128  +0.170  +0.200  +0.198  +0.132  +0.012  -0.090  -0.141  -0.132  +0.155  +0.178  +0.164  +1.000  -0.025  -0.129  -0.192  -0.179  +0.138  +0.150  +0.134  +0.030  -0.123  -0.175  -0.211  -0.220  +0.113  +0.097  +0.055  -0.003  -0.152  -0.206  -0.223  -0.211  +0.064  +0.048  -0.007  -0.070  -0.150  -0.201  -0.205  -0.190  +0.027  +0.037  -0.009  -0.077  -0.138  -0.176  -0.184  -0.157\n",
      "                         :   var28:  -0.070  -0.076  +0.006  +0.071  +0.123  +0.161  +0.151  +0.124  -0.105  -0.096  -0.027  +0.041  +0.142  +0.184  +0.185  +0.149  -0.131  -0.141  -0.058  +0.038  +0.125  +0.212  +0.184  +0.170  -0.158  -0.174  -0.140  -0.025  +1.000  +0.153  +0.183  +0.139  -0.191  -0.219  -0.167  -0.092  +0.042  +0.122  +0.143  +0.138  -0.183  -0.234  -0.211  -0.136  -0.001  +0.082  +0.120  +0.090  -0.192  -0.222  -0.193  -0.147  -0.031  +0.011  +0.088  +0.075  -0.185  -0.184  -0.165  -0.154  -0.079  -0.023  +0.044  +0.061\n",
      "                         :   var29:  -0.217  -0.204  -0.129  -0.001  +0.146  +0.258  +0.293  +0.256  -0.271  -0.261  -0.195  -0.017  +0.158  +0.320  +0.344  +0.319  -0.319  -0.327  -0.255  -0.054  +0.193  +0.362  +0.396  +0.338  -0.389  -0.394  -0.336  -0.129  +0.153  +1.000  +0.426  +0.369  -0.385  -0.429  -0.386  -0.199  +0.138  +0.319  +0.369  +0.355  -0.374  -0.397  -0.353  -0.160  +0.059  +0.260  +0.333  +0.307  -0.340  -0.352  -0.333  -0.178  +0.019  +0.166  +0.254  +0.243  -0.270  -0.273  -0.275  -0.159  -0.016  +0.107  +0.175  +0.172\n",
      "                         :   var30:  -0.250  -0.269  -0.193  -0.046  +0.124  +0.257  +0.315  +0.291  -0.314  -0.337  -0.261  -0.085  +0.141  +0.338  +0.390  +0.378  -0.374  -0.412  -0.337  -0.123  +0.168  +0.383  +0.449  +0.435  -0.431  -0.460  -0.408  -0.192  +0.183  +0.426  +1.000  +0.455  -0.436  -0.485  -0.450  -0.210  +0.156  +0.385  +0.474  +0.450  -0.419  -0.465  -0.390  -0.209  +0.099  +0.316  +0.401  +0.404  -0.349  -0.393  -0.341  -0.195  +0.066  +0.232  +0.327  +0.317  -0.281  -0.293  -0.262  -0.157  +0.017  +0.164  +0.242  +0.244\n",
      "                         :   var31:  -0.221  -0.239  -0.186  -0.053  +0.090  +0.218  +0.284  +0.252  -0.280  -0.306  -0.260  -0.095  +0.122  +0.282  +0.342  +0.345  -0.326  -0.361  -0.300  -0.121  +0.136  +0.329  +0.407  +0.389  -0.351  -0.388  -0.364  -0.179  +0.139  +0.369  +0.455  +1.000  -0.381  -0.419  -0.398  -0.196  +0.126  +0.329  +0.417  +0.390  -0.347  -0.400  -0.336  -0.164  +0.083  +0.300  +0.382  +0.358  -0.314  -0.333  -0.288  -0.155  +0.051  +0.222  +0.296  +0.290  -0.231  -0.258  -0.215  -0.138  +0.034  +0.154  +0.236  +0.203\n",
      "                         :   var32:  +0.227  +0.241  +0.152  +0.025  -0.114  -0.225  -0.281  -0.224  +0.283  +0.297  +0.221  +0.043  -0.147  -0.294  -0.332  -0.301  +0.364  +0.372  +0.297  +0.093  -0.172  -0.345  -0.397  -0.371  +0.393  +0.416  +0.340  +0.138  -0.191  -0.385  -0.436  -0.381  +1.000  +0.434  +0.371  +0.158  -0.183  -0.368  -0.413  -0.372  +0.382  +0.418  +0.356  +0.123  -0.130  -0.323  -0.381  -0.349  +0.347  +0.342  +0.274  +0.118  -0.110  -0.238  -0.306  -0.291  +0.272  +0.284  +0.206  +0.102  -0.065  -0.180  -0.243  -0.233\n",
      "                         :   var33:  +0.233  +0.229  +0.158  +0.014  -0.148  -0.268  -0.289  -0.284  +0.305  +0.329  +0.241  +0.052  -0.178  -0.340  -0.369  -0.349  +0.379  +0.413  +0.310  +0.083  -0.188  -0.388  -0.439  -0.410  +0.436  +0.447  +0.388  +0.150  -0.219  -0.429  -0.485  -0.419  +0.434  +1.000  +0.410  +0.182  -0.178  -0.389  -0.457  -0.415  +0.418  +0.451  +0.373  +0.174  -0.109  -0.355  -0.426  -0.364  +0.366  +0.379  +0.331  +0.166  -0.092  -0.258  -0.341  -0.298  +0.287  +0.299  +0.260  +0.126  -0.037  -0.181  -0.259  -0.238\n",
      "                         :   var34:  +0.188  +0.188  +0.122  -0.035  -0.161  -0.261  -0.304  -0.261  +0.227  +0.248  +0.169  +0.010  -0.180  -0.337  -0.353  -0.318  +0.292  +0.304  +0.258  +0.071  -0.191  -0.359  -0.410  -0.376  +0.360  +0.371  +0.332  +0.134  -0.167  -0.386  -0.450  -0.398  +0.371  +0.410  +1.000  +0.181  -0.133  -0.339  -0.404  -0.360  +0.388  +0.409  +0.357  +0.177  -0.075  -0.290  -0.339  -0.322  +0.309  +0.338  +0.314  +0.183  -0.049  -0.179  -0.261  -0.267  +0.249  +0.289  +0.252  +0.152  -0.001  -0.123  -0.201  -0.194\n",
      "                         :   var35:  +0.026  +0.023  -0.026  -0.076  -0.142  -0.180  -0.190  -0.158  +0.064  +0.055  +0.019  -0.050  -0.169  -0.202  -0.201  -0.185  +0.105  +0.090  +0.051  -0.020  -0.114  -0.197  -0.239  -0.208  +0.106  +0.145  +0.139  +0.030  -0.092  -0.199  -0.210  -0.196  +0.158  +0.182  +0.181  +1.000  -0.014  -0.148  -0.180  -0.184  +0.181  +0.196  +0.198  +0.135  +0.033  -0.089  -0.134  -0.123  +0.167  +0.200  +0.190  +0.145  +0.055  -0.043  -0.093  -0.094  +0.135  +0.160  +0.168  +0.141  +0.065  -0.001  -0.049  -0.049\n",
      "                         :   var36:  -0.167  -0.196  -0.199  -0.134  -0.084  +0.011  +0.026  +0.046  -0.190  -0.210  -0.199  -0.140  -0.056  +0.013  +0.073  +0.060  -0.193  -0.236  -0.201  -0.124  -0.016  +0.065  +0.118  +0.095  -0.212  -0.207  -0.186  -0.123  +0.042  +0.138  +0.156  +0.126  -0.183  -0.178  -0.133  -0.014  +1.000  +0.180  +0.184  +0.158  -0.167  -0.133  -0.087  +0.025  +0.140  +0.211  +0.190  +0.174  -0.103  -0.083  -0.022  +0.056  +0.158  +0.184  +0.196  +0.163  -0.083  -0.045  +0.001  +0.081  +0.139  +0.166  +0.171  +0.144\n",
      "                         :   var37:  -0.252  -0.292  -0.271  -0.153  -0.039  +0.109  +0.145  +0.166  -0.323  -0.354  -0.324  -0.193  -0.006  +0.175  +0.236  +0.256  -0.365  -0.391  -0.345  -0.191  +0.063  +0.246  +0.298  +0.299  -0.388  -0.405  -0.376  -0.175  +0.122  +0.319  +0.385  +0.329  -0.368  -0.389  -0.339  -0.148  +0.180  +1.000  +0.392  +0.375  -0.322  -0.329  -0.246  -0.042  +0.173  +0.374  +0.388  +0.363  -0.247  -0.259  -0.176  -0.007  +0.206  +0.317  +0.352  +0.328  -0.189  -0.194  -0.136  -0.003  +0.154  +0.268  +0.303  +0.254\n",
      "                         :   var38:  -0.301  -0.316  -0.259  -0.145  +0.003  +0.155  +0.230  +0.216  -0.355  -0.384  -0.340  -0.181  +0.034  +0.224  +0.290  +0.309  -0.408  -0.430  -0.384  -0.211  +0.079  +0.293  +0.385  +0.371  -0.425  -0.456  -0.423  -0.211  +0.143  +0.369  +0.474  +0.417  -0.413  -0.457  -0.404  -0.180  +0.184  +0.392  +1.000  +0.444  -0.391  -0.404  -0.318  -0.114  +0.175  +0.393  +0.458  +0.415  -0.322  -0.332  -0.247  -0.079  +0.195  +0.346  +0.410  +0.369  -0.235  -0.230  -0.177  -0.039  +0.137  +0.269  +0.344  +0.291\n",
      "                         :   var39:  -0.233  -0.270  -0.237  -0.124  +0.014  +0.153  +0.222  +0.219  -0.297  -0.347  -0.310  -0.167  +0.043  +0.219  +0.283  +0.297  -0.362  -0.402  -0.336  -0.170  +0.090  +0.296  +0.379  +0.357  -0.392  -0.422  -0.386  -0.220  +0.138  +0.355  +0.450  +0.390  -0.372  -0.415  -0.360  -0.184  +0.158  +0.375  +0.444  +1.000  -0.357  -0.383  -0.302  -0.124  +0.175  +0.345  +0.425  +0.393  -0.285  -0.322  -0.230  -0.092  +0.150  +0.286  +0.372  +0.341  -0.214  -0.234  -0.168  -0.040  +0.114  +0.245  +0.307  +0.262\n",
      "                         :   var40:  +0.177  +0.154  +0.070  -0.030  -0.165  -0.260  -0.268  -0.244  +0.227  +0.226  +0.158  +0.003  -0.190  -0.305  -0.324  -0.303  +0.306  +0.306  +0.227  +0.027  -0.215  -0.365  -0.378  -0.330  +0.354  +0.381  +0.312  +0.113  -0.183  -0.374  -0.419  -0.347  +0.382  +0.418  +0.388  +0.181  -0.167  -0.322  -0.391  -0.357  +1.000  +0.427  +0.371  +0.190  -0.082  -0.265  -0.345  -0.330  +0.363  +0.381  +0.327  +0.185  -0.052  -0.189  -0.277  -0.269  +0.278  +0.300  +0.279  +0.157  -0.010  -0.116  -0.220  -0.197\n",
      "                         :   var41:  +0.155  +0.141  +0.037  -0.103  -0.217  -0.309  -0.331  -0.269  +0.215  +0.221  +0.128  -0.050  -0.246  -0.367  -0.392  -0.343  +0.303  +0.301  +0.217  -0.007  -0.253  -0.411  -0.427  -0.393  +0.358  +0.365  +0.333  +0.097  -0.234  -0.397  -0.465  -0.400  +0.418  +0.451  +0.409  +0.196  -0.133  -0.329  -0.404  -0.383  +0.427  +1.000  +0.418  +0.243  -0.034  -0.263  -0.348  -0.337  +0.397  +0.439  +0.398  +0.263  +0.001  -0.165  -0.276  -0.264  +0.314  +0.354  +0.331  +0.233  +0.045  -0.102  -0.198  -0.192\n",
      "                         :   var42:  +0.078  +0.050  -0.033  -0.160  -0.256  -0.331  -0.342  -0.278  +0.139  +0.113  +0.016  -0.124  -0.293  -0.372  -0.377  -0.330  +0.207  +0.201  +0.113  -0.043  -0.255  -0.379  -0.395  -0.355  +0.275  +0.281  +0.246  +0.055  -0.211  -0.353  -0.390  -0.336  +0.356  +0.373  +0.357  +0.198  -0.087  -0.246  -0.318  -0.302  +0.371  +0.418  +1.000  +0.260  +0.044  -0.135  -0.258  -0.252  +0.355  +0.414  +0.399  +0.290  +0.092  -0.048  -0.175  -0.197  +0.315  +0.363  +0.348  +0.269  +0.133  -0.006  -0.087  -0.112\n",
      "                         :   var43:  -0.048  -0.135  -0.190  -0.238  -0.283  -0.273  -0.263  -0.208  -0.058  -0.079  -0.141  -0.222  -0.298  -0.304  -0.270  -0.221  +0.012  -0.012  -0.050  -0.168  -0.222  -0.263  -0.238  -0.220  +0.068  +0.070  +0.062  -0.003  -0.136  -0.160  -0.209  -0.164  +0.123  +0.174  +0.177  +0.135  +0.025  -0.042  -0.114  -0.124  +0.190  +0.243  +0.260  +1.000  +0.157  +0.042  -0.054  -0.072  +0.195  +0.269  +0.308  +0.292  +0.212  +0.135  +0.023  -0.018  +0.196  +0.251  +0.279  +0.316  +0.224  +0.152  +0.074  +0.029\n",
      "                         :   var44:  -0.203  -0.282  -0.284  -0.293  -0.247  -0.177  -0.102  -0.062  -0.217  -0.304  -0.327  -0.308  -0.254  -0.116  -0.062  -0.037  -0.211  -0.252  -0.293  -0.252  -0.138  -0.057  +0.017  +0.030  -0.203  -0.195  -0.185  -0.152  -0.001  +0.059  +0.099  +0.083  -0.130  -0.109  -0.075  +0.033  +0.140  +0.173  +0.175  +0.175  -0.082  -0.034  +0.044  +0.157  +1.000  +0.274  +0.232  +0.203  -0.028  +0.032  +0.110  +0.221  +0.316  +0.316  +0.260  +0.227  +0.037  +0.088  +0.135  +0.236  +0.300  +0.314  +0.267  +0.199\n",
      "                         :   var45:  -0.273  -0.348  -0.335  -0.259  -0.149  -0.027  +0.056  +0.081  -0.345  -0.397  -0.397  -0.282  -0.128  +0.042  +0.107  +0.163  -0.360  -0.401  -0.411  -0.286  -0.069  +0.136  +0.228  +0.239  -0.361  -0.398  -0.363  -0.206  +0.082  +0.260  +0.316  +0.300  -0.323  -0.355  -0.290  -0.089  +0.211  +0.374  +0.393  +0.345  -0.265  -0.263  -0.135  +0.042  +0.274  +1.000  +0.426  +0.373  -0.194  -0.174  -0.075  +0.093  +0.309  +0.397  +0.414  +0.369  -0.127  -0.088  -0.009  +0.126  +0.274  +0.362  +0.374  +0.305\n",
      "                         :   var46:  -0.293  -0.323  -0.297  -0.203  -0.093  +0.062  +0.152  +0.158  -0.351  -0.396  -0.376  -0.235  -0.054  +0.142  +0.212  +0.240  -0.377  -0.417  -0.393  -0.249  +0.018  +0.212  +0.308  +0.307  -0.392  -0.432  -0.387  -0.223  +0.120  +0.333  +0.401  +0.382  -0.381  -0.426  -0.339  -0.134  +0.190  +0.388  +0.458  +0.425  -0.345  -0.348  -0.258  -0.054  +0.232  +0.426  +1.000  +0.415  -0.264  -0.265  -0.160  +0.009  +0.250  +0.371  +0.423  +0.384  -0.179  -0.164  -0.100  +0.039  +0.218  +0.326  +0.366  +0.320\n",
      "                         :   var47:  -0.241  -0.294  -0.267  -0.187  -0.061  +0.079  +0.131  +0.153  -0.296  -0.354  -0.340  -0.223  -0.029  +0.154  +0.206  +0.248  -0.339  -0.405  -0.365  -0.228  +0.032  +0.220  +0.301  +0.312  -0.377  -0.384  -0.362  -0.211  +0.090  +0.307  +0.404  +0.358  -0.349  -0.364  -0.322  -0.123  +0.174  +0.363  +0.415  +0.393  -0.330  -0.337  -0.252  -0.072  +0.203  +0.373  +0.415  +1.000  -0.249  -0.252  -0.175  -0.011  +0.209  +0.316  +0.395  +0.351  -0.187  -0.171  -0.112  +0.008  +0.167  +0.292  +0.338  +0.286\n",
      "                         :   var48:  +0.110  +0.065  -0.002  -0.108  -0.222  -0.247  -0.274  -0.211  +0.175  +0.140  +0.069  -0.077  -0.237  -0.336  -0.308  -0.260  +0.233  +0.234  +0.135  -0.032  -0.225  -0.336  -0.338  -0.322  +0.278  +0.308  +0.239  +0.064  -0.192  -0.340  -0.349  -0.314  +0.347  +0.366  +0.309  +0.167  -0.103  -0.247  -0.322  -0.285  +0.363  +0.397  +0.355  +0.195  -0.028  -0.194  -0.264  -0.249  +1.000  +0.377  +0.353  +0.216  +0.023  -0.114  -0.191  -0.207  +0.291  +0.326  +0.299  +0.222  +0.074  -0.043  -0.122  -0.127\n",
      "                         :   var49:  +0.075  +0.037  -0.043  -0.170  -0.253  -0.316  -0.318  -0.265  +0.132  +0.092  +0.025  -0.140  -0.292  -0.362  -0.378  -0.312  +0.221  +0.222  +0.117  -0.064  -0.272  -0.386  -0.377  -0.343  +0.285  +0.289  +0.245  +0.048  -0.222  -0.352  -0.393  -0.333  +0.342  +0.379  +0.338  +0.200  -0.083  -0.259  -0.332  -0.322  +0.381  +0.439  +0.414  +0.269  +0.032  -0.174  -0.265  -0.252  +0.377  +1.000  +0.419  +0.276  +0.083  -0.064  -0.178  -0.189  +0.333  +0.376  +0.373  +0.285  +0.125  -0.006  -0.106  -0.128\n",
      "                         :   var50:  +0.018  -0.056  -0.136  -0.253  -0.329  -0.356  -0.331  -0.250  +0.044  +0.014  -0.065  -0.214  -0.331  -0.384  -0.361  -0.319  +0.111  +0.130  +0.013  -0.138  -0.309  -0.394  -0.370  -0.310  +0.215  +0.214  +0.183  -0.007  -0.193  -0.333  -0.341  -0.288  +0.274  +0.331  +0.314  +0.190  -0.022  -0.176  -0.247  -0.230  +0.327  +0.398  +0.399  +0.308  +0.110  -0.075  -0.160  -0.175  +0.353  +0.419  +1.000  +0.355  +0.190  +0.040  -0.072  -0.128  +0.306  +0.352  +0.411  +0.361  +0.233  +0.071  -0.012  -0.066\n",
      "                         :   var51:  -0.103  -0.181  -0.232  -0.329  -0.365  -0.323  -0.275  -0.207  -0.099  -0.149  -0.212  -0.312  -0.342  -0.325  -0.284  -0.217  -0.027  -0.064  -0.133  -0.226  -0.282  -0.287  -0.243  -0.207  +0.024  +0.046  +0.026  -0.070  -0.147  -0.178  -0.195  -0.155  +0.118  +0.166  +0.183  +0.145  +0.056  -0.007  -0.079  -0.092  +0.185  +0.263  +0.290  +0.292  +0.221  +0.093  +0.009  -0.011  +0.216  +0.276  +0.355  +1.000  +0.293  +0.193  +0.099  +0.063  +0.229  +0.302  +0.340  +0.383  +0.329  +0.222  +0.140  +0.070\n",
      "                         :   var52:  -0.219  -0.303  -0.344  -0.361  -0.315  -0.248  -0.140  -0.094  -0.258  -0.332  -0.344  -0.348  -0.295  -0.188  -0.115  -0.056  -0.222  -0.265  -0.323  -0.289  -0.222  -0.113  -0.039  -0.018  -0.188  -0.175  -0.179  -0.150  -0.031  +0.019  +0.066  +0.051  -0.110  -0.092  -0.049  +0.055  +0.158  +0.206  +0.195  +0.150  -0.052  +0.001  +0.092  +0.212  +0.316  +0.309  +0.250  +0.209  +0.023  +0.083  +0.190  +0.293  +1.000  +0.352  +0.288  +0.229  +0.075  +0.127  +0.217  +0.317  +0.362  +0.351  +0.320  +0.223\n",
      "                         :   var53:  -0.261  -0.325  -0.364  -0.323  -0.247  -0.122  -0.033  +0.003  -0.296  -0.379  -0.396  -0.340  -0.218  -0.065  +0.005  +0.053  -0.322  -0.361  -0.380  -0.304  -0.145  +0.022  +0.122  +0.131  -0.298  -0.318  -0.312  -0.201  +0.011  +0.166  +0.232  +0.222  -0.238  -0.258  -0.179  -0.043  +0.184  +0.317  +0.346  +0.286  -0.189  -0.165  -0.048  +0.135  +0.316  +0.397  +0.371  +0.316  -0.114  -0.064  +0.040  +0.193  +0.352  +1.000  +0.407  +0.333  -0.035  +0.010  +0.076  +0.223  +0.337  +0.388  +0.377  +0.276\n",
      "                         :   var54:  -0.255  -0.327  -0.308  -0.263  -0.155  -0.032  +0.065  +0.067  -0.320  -0.385  -0.388  -0.290  -0.131  +0.026  +0.126  +0.153  -0.340  -0.406  -0.386  -0.276  -0.067  +0.115  +0.233  +0.247  -0.348  -0.385  -0.353  -0.205  +0.088  +0.254  +0.327  +0.296  -0.306  -0.341  -0.261  -0.093  +0.196  +0.352  +0.410  +0.372  -0.277  -0.276  -0.175  +0.023  +0.260  +0.414  +0.423  +0.395  -0.191  -0.178  -0.072  +0.099  +0.288  +0.407  +1.000  +0.375  -0.117  -0.092  -0.024  +0.112  +0.275  +0.353  +0.377  +0.320\n",
      "                         :   var55:  -0.226  -0.266  -0.262  -0.224  -0.111  -0.011  +0.082  +0.095  -0.257  -0.313  -0.317  -0.230  -0.096  +0.051  +0.120  +0.164  -0.316  -0.337  -0.336  -0.240  -0.025  +0.160  +0.210  +0.241  -0.312  -0.341  -0.331  -0.190  +0.075  +0.243  +0.317  +0.290  -0.291  -0.298  -0.267  -0.094  +0.163  +0.328  +0.369  +0.341  -0.269  -0.264  -0.197  -0.018  +0.227  +0.369  +0.384  +0.351  -0.207  -0.189  -0.128  +0.063  +0.229  +0.333  +0.375  +1.000  -0.121  -0.115  -0.057  +0.070  +0.217  +0.309  +0.321  +0.278\n",
      "                         :   var56:  +0.059  +0.036  -0.054  -0.129  -0.194  -0.213  -0.211  -0.194  +0.099  +0.074  +0.002  -0.105  -0.239  -0.269  -0.253  -0.218  +0.165  +0.162  +0.074  -0.073  -0.205  -0.283  -0.288  -0.249  +0.214  +0.206  +0.176  +0.027  -0.185  -0.270  -0.281  -0.231  +0.272  +0.287  +0.249  +0.135  -0.083  -0.189  -0.235  -0.214  +0.278  +0.314  +0.315  +0.196  +0.037  -0.127  -0.179  -0.187  +0.291  +0.333  +0.306  +0.229  +0.075  -0.035  -0.117  -0.121  +1.000  +0.299  +0.272  +0.215  +0.107  -0.015  -0.080  -0.087\n",
      "                         :   var57:  +0.014  +0.000  -0.080  -0.213  -0.263  -0.295  -0.268  -0.202  +0.062  +0.041  -0.055  -0.172  -0.289  -0.326  -0.304  -0.249  +0.136  +0.104  +0.021  -0.106  -0.262  -0.338  -0.316  -0.289  +0.210  +0.207  +0.168  +0.037  -0.184  -0.273  -0.293  -0.258  +0.284  +0.299  +0.289  +0.160  -0.045  -0.194  -0.230  -0.234  +0.300  +0.354  +0.363  +0.251  +0.088  -0.088  -0.164  -0.171  +0.326  +0.376  +0.352  +0.302  +0.127  +0.010  -0.092  -0.115  +0.299  +1.000  +0.340  +0.287  +0.174  +0.046  -0.039  -0.075\n",
      "                         :   var58:  -0.029  -0.084  -0.163  -0.274  -0.321  -0.321  -0.308  -0.214  -0.001  -0.051  -0.125  -0.229  -0.338  -0.347  -0.320  -0.252  +0.048  +0.045  -0.033  -0.163  -0.268  -0.324  -0.302  -0.269  +0.138  +0.152  +0.121  -0.009  -0.165  -0.275  -0.262  -0.215  +0.206  +0.260  +0.252  +0.168  +0.001  -0.136  -0.177  -0.168  +0.279  +0.331  +0.348  +0.279  +0.135  -0.009  -0.100  -0.112  +0.299  +0.373  +0.411  +0.340  +0.217  +0.076  -0.024  -0.057  +0.272  +0.340  +1.000  +0.342  +0.244  +0.135  +0.042  -0.012\n",
      "                         :   var59:  -0.113  -0.205  -0.270  -0.333  -0.346  -0.317  -0.274  -0.198  -0.088  -0.183  -0.253  -0.315  -0.371  -0.326  -0.277  -0.182  -0.055  -0.077  -0.155  -0.261  -0.284  -0.260  -0.239  -0.177  +0.001  +0.027  -0.005  -0.077  -0.154  -0.159  -0.157  -0.138  +0.102  +0.126  +0.152  +0.141  +0.081  -0.003  -0.039  -0.040  +0.157  +0.233  +0.269  +0.316  +0.236  +0.126  +0.039  +0.008  +0.222  +0.285  +0.361  +0.383  +0.317  +0.223  +0.112  +0.070  +0.215  +0.287  +0.342  +1.000  +0.347  +0.265  +0.167  +0.083\n",
      "                         :   var60:  -0.197  -0.269  -0.307  -0.342  -0.331  -0.261  -0.174  -0.127  -0.221  -0.290  -0.326  -0.349  -0.313  -0.236  -0.164  -0.103  -0.188  -0.213  -0.286  -0.313  -0.227  -0.154  -0.097  -0.050  -0.132  -0.163  -0.153  -0.138  -0.079  -0.016  +0.017  +0.034  -0.065  -0.037  -0.001  +0.065  +0.139  +0.154  +0.137  +0.114  -0.010  +0.045  +0.133  +0.224  +0.300  +0.274  +0.218  +0.167  +0.074  +0.125  +0.233  +0.329  +0.362  +0.337  +0.275  +0.217  +0.107  +0.174  +0.244  +0.347  +1.000  +0.354  +0.288  +0.187\n",
      "                         :   var61:  -0.236  -0.302  -0.353  -0.308  -0.275  -0.162  -0.090  -0.034  -0.277  -0.345  -0.365  -0.343  -0.272  -0.124  -0.044  +0.026  -0.263  -0.333  -0.364  -0.308  -0.156  -0.015  +0.042  +0.089  -0.254  -0.270  -0.239  -0.176  -0.023  +0.107  +0.164  +0.154  -0.180  -0.181  -0.123  -0.001  +0.166  +0.268  +0.269  +0.245  -0.116  -0.102  -0.006  +0.152  +0.314  +0.362  +0.326  +0.292  -0.043  -0.006  +0.071  +0.222  +0.351  +0.388  +0.353  +0.309  -0.015  +0.046  +0.135  +0.265  +0.354  +1.000  +0.349  +0.282\n",
      "                         :   var62:  -0.220  -0.290  -0.303  -0.282  -0.179  -0.101  -0.001  +0.033  -0.278  -0.354  -0.345  -0.293  -0.188  -0.028  +0.044  +0.101  -0.289  -0.331  -0.347  -0.299  -0.108  +0.061  +0.142  +0.139  -0.276  -0.313  -0.286  -0.184  +0.044  +0.175  +0.242  +0.236  -0.243  -0.259  -0.201  -0.049  +0.171  +0.303  +0.344  +0.307  -0.220  -0.198  -0.087  +0.074  +0.267  +0.374  +0.366  +0.338  -0.122  -0.106  -0.012  +0.140  +0.320  +0.377  +0.377  +0.321  -0.080  -0.039  +0.042  +0.167  +0.288  +0.349  +1.000  +0.287\n",
      "                         :   var63:  -0.184  -0.233  -0.206  -0.186  -0.113  -0.042  +0.009  +0.036  -0.185  -0.264  -0.288  -0.217  -0.106  -0.019  +0.047  +0.103  -0.224  -0.281  -0.255  -0.209  -0.072  +0.069  +0.141  +0.154  -0.253  -0.257  -0.260  -0.157  +0.061  +0.172  +0.244  +0.203  -0.233  -0.238  -0.194  -0.049  +0.144  +0.254  +0.291  +0.262  -0.197  -0.192  -0.112  +0.029  +0.199  +0.305  +0.320  +0.286  -0.127  -0.128  -0.066  +0.070  +0.223  +0.276  +0.320  +0.278  -0.087  -0.075  -0.012  +0.083  +0.187  +0.282  +0.287  +1.000\n",
      "                         : ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataSetInfo              : Correlation matrix (Background):\n",
      "                         : ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "                         :             var0    var1    var2    var3    var4    var5    var6    var7    var8    var9   var10   var11   var12   var13   var14   var15   var16   var17   var18   var19   var20   var21   var22   var23   var24   var25   var26   var27   var28   var29   var30   var31   var32   var33   var34   var35   var36   var37   var38   var39   var40   var41   var42   var43   var44   var45   var46   var47   var48   var49   var50   var51   var52   var53   var54   var55   var56   var57   var58   var59   var60   var61   var62   var63\n",
      "                         :    var0:  +1.000  +0.279  +0.265  +0.252  +0.153  +0.067  +0.016  -0.014  +0.301  +0.315  +0.310  +0.259  +0.164  +0.044  -0.000  -0.080  +0.281  +0.302  +0.311  +0.223  +0.099  -0.038  -0.120  -0.112  +0.254  +0.256  +0.243  +0.126  -0.008  -0.129  -0.168  -0.184  +0.158  +0.138  +0.081  -0.024  -0.160  -0.222  -0.260  -0.223  +0.093  +0.040  -0.028  -0.120  -0.226  -0.277  -0.276  -0.231  +0.011  -0.016  -0.072  -0.181  -0.243  -0.265  -0.263  -0.220  -0.015  -0.087  -0.123  -0.168  -0.228  -0.231  -0.225  -0.170\n",
      "                         :    var1:  +0.279  +1.000  +0.343  +0.304  +0.232  +0.126  +0.052  +0.009  +0.336  +0.370  +0.367  +0.327  +0.229  +0.093  +0.008  -0.037  +0.303  +0.340  +0.325  +0.288  +0.158  +0.017  -0.080  -0.106  +0.258  +0.269  +0.238  +0.154  +0.015  -0.116  -0.166  -0.186  +0.154  +0.123  +0.066  -0.045  -0.189  -0.248  -0.273  -0.224  +0.070  +0.013  -0.066  -0.185  -0.281  -0.309  -0.340  -0.284  -0.013  -0.067  -0.145  -0.236  -0.307  -0.329  -0.322  -0.268  -0.053  -0.123  -0.166  -0.231  -0.295  -0.284  -0.262  -0.217\n",
      "                         :    var2:  +0.265  +0.343  +1.000  +0.390  +0.314  +0.252  +0.147  +0.069  +0.313  +0.387  +0.416  +0.401  +0.339  +0.211  +0.111  +0.042  +0.292  +0.330  +0.352  +0.323  +0.245  +0.115  +0.035  -0.028  +0.218  +0.213  +0.213  +0.136  +0.069  -0.053  -0.105  -0.112  +0.097  +0.075  +0.010  -0.082  -0.197  -0.258  -0.219  -0.189  -0.009  -0.067  -0.157  -0.258  -0.341  -0.343  -0.311  -0.269  -0.078  -0.164  -0.228  -0.327  -0.382  -0.360  -0.340  -0.267  -0.123  -0.200  -0.252  -0.330  -0.363  -0.333  -0.313  -0.257\n",
      "                         :    var3:  +0.252  +0.304  +0.390  +1.000  +0.360  +0.310  +0.225  +0.186  +0.229  +0.326  +0.383  +0.395  +0.382  +0.279  +0.216  +0.131  +0.198  +0.254  +0.313  +0.331  +0.300  +0.209  +0.120  +0.066  +0.123  +0.118  +0.144  +0.136  +0.106  +0.031  +0.001  -0.036  -0.016  -0.034  -0.075  -0.109  -0.146  -0.167  -0.148  -0.114  -0.095  -0.145  -0.243  -0.285  -0.341  -0.307  -0.268  -0.209  -0.157  -0.221  -0.309  -0.377  -0.388  -0.360  -0.310  -0.221  -0.179  -0.255  -0.317  -0.359  -0.378  -0.340  -0.298  -0.210\n",
      "                         :    var4:  +0.153  +0.232  +0.314  +0.360  +1.000  +0.360  +0.311  +0.235  +0.157  +0.216  +0.306  +0.362  +0.399  +0.363  +0.309  +0.250  +0.069  +0.128  +0.216  +0.287  +0.344  +0.302  +0.254  +0.184  -0.012  +0.016  +0.041  +0.099  +0.135  +0.141  +0.134  +0.096  -0.128  -0.156  -0.147  -0.153  -0.133  -0.097  -0.032  -0.007  -0.185  -0.233  -0.317  -0.320  -0.313  -0.223  -0.168  -0.111  -0.218  -0.289  -0.337  -0.386  -0.355  -0.306  -0.238  -0.165  -0.207  -0.281  -0.347  -0.357  -0.361  -0.318  -0.241  -0.177\n",
      "                         :    var5:  +0.067  +0.126  +0.252  +0.310  +0.360  +1.000  +0.344  +0.279  +0.042  +0.133  +0.203  +0.299  +0.385  +0.374  +0.356  +0.298  -0.024  +0.047  +0.120  +0.223  +0.323  +0.344  +0.300  +0.261  -0.098  -0.102  -0.047  +0.061  +0.171  +0.210  +0.207  +0.192  -0.202  -0.208  -0.222  -0.171  -0.090  -0.001  +0.055  +0.093  -0.249  -0.296  -0.328  -0.314  -0.253  -0.159  -0.090  -0.013  -0.263  -0.323  -0.360  -0.373  -0.299  -0.222  -0.158  -0.077  -0.226  -0.284  -0.349  -0.314  -0.320  -0.253  -0.173  -0.105\n",
      "                         :    var6:  +0.016  +0.052  +0.147  +0.225  +0.311  +0.344  +1.000  +0.292  -0.024  +0.031  +0.121  +0.221  +0.338  +0.375  +0.370  +0.338  -0.085  -0.051  +0.018  +0.173  +0.298  +0.344  +0.347  +0.309  -0.177  -0.165  -0.107  +0.001  +0.178  +0.236  +0.263  +0.244  -0.240  -0.260  -0.246  -0.166  -0.056  +0.063  +0.124  +0.152  -0.272  -0.313  -0.316  -0.295  -0.191  -0.081  +0.012  +0.045  -0.248  -0.308  -0.331  -0.306  -0.230  -0.147  -0.086  -0.012  -0.217  -0.270  -0.315  -0.292  -0.254  -0.190  -0.101  -0.061\n",
      "                         :    var7:  -0.014  +0.009  +0.069  +0.186  +0.235  +0.279  +0.292  +1.000  -0.060  -0.014  +0.049  +0.156  +0.264  +0.285  +0.313  +0.290  -0.098  -0.067  -0.004  +0.082  +0.224  +0.290  +0.315  +0.271  -0.165  -0.177  -0.107  +0.003  +0.131  +0.226  +0.254  +0.243  -0.225  -0.259  -0.233  -0.141  -0.046  +0.094  +0.142  +0.178  -0.240  -0.267  -0.277  -0.239  -0.128  -0.030  +0.076  +0.102  -0.228  -0.259  -0.282  -0.254  -0.169  -0.101  -0.038  +0.013  -0.192  -0.219  -0.257  -0.215  -0.182  -0.105  -0.062  -0.002\n",
      "                         :    var8:  +0.301  +0.336  +0.313  +0.229  +0.157  +0.042  -0.024  -0.060  +1.000  +0.387  +0.356  +0.271  +0.121  +0.019  -0.079  -0.130  +0.344  +0.363  +0.344  +0.230  +0.088  -0.073  -0.159  -0.181  +0.328  +0.336  +0.283  +0.162  -0.046  -0.180  -0.238  -0.247  +0.256  +0.207  +0.150  +0.001  -0.176  -0.294  -0.308  -0.285  +0.173  +0.128  +0.028  -0.104  -0.259  -0.330  -0.346  -0.309  +0.075  +0.006  -0.057  -0.177  -0.254  -0.320  -0.317  -0.259  +0.005  -0.036  -0.086  -0.187  -0.250  -0.271  -0.280  -0.227\n",
      "                         :    var9:  +0.315  +0.370  +0.387  +0.326  +0.216  +0.133  +0.031  -0.014  +0.387  +1.000  +0.433  +0.351  +0.217  +0.074  -0.014  -0.075  +0.365  +0.400  +0.399  +0.321  +0.142  -0.009  -0.117  -0.157  +0.331  +0.341  +0.303  +0.180  +0.006  -0.161  -0.218  -0.236  +0.207  +0.205  +0.118  -0.001  -0.236  -0.321  -0.333  -0.299  +0.114  +0.055  -0.024  -0.165  -0.315  -0.391  -0.385  -0.336  +0.033  -0.045  -0.122  -0.248  -0.336  -0.381  -0.399  -0.308  -0.032  -0.087  -0.159  -0.249  -0.319  -0.342  -0.334  -0.254\n",
      "                         :   var10:  +0.310  +0.367  +0.416  +0.383  +0.306  +0.203  +0.121  +0.049  +0.356  +0.433  +1.000  +0.417  +0.307  +0.177  +0.067  -0.010  +0.321  +0.378  +0.397  +0.367  +0.231  +0.075  -0.019  -0.072  +0.269  +0.295  +0.263  +0.178  +0.059  -0.088  -0.163  -0.167  +0.143  +0.115  +0.058  -0.084  -0.193  -0.286  -0.294  -0.263  +0.046  -0.012  -0.130  -0.233  -0.345  -0.392  -0.382  -0.317  -0.037  -0.124  -0.206  -0.324  -0.395  -0.398  -0.383  -0.308  -0.092  -0.183  -0.243  -0.315  -0.378  -0.379  -0.345  -0.261\n",
      "                         :   var11:  +0.259  +0.327  +0.401  +0.395  +0.362  +0.299  +0.221  +0.156  +0.271  +0.351  +0.417  +1.000  +0.374  +0.287  +0.191  +0.123  +0.214  +0.281  +0.334  +0.329  +0.293  +0.195  +0.122  +0.053  +0.152  +0.159  +0.179  +0.163  +0.098  +0.034  -0.002  -0.033  +0.005  -0.005  -0.057  -0.115  -0.162  -0.191  -0.177  -0.138  -0.087  -0.143  -0.230  -0.295  -0.362  -0.318  -0.295  -0.227  -0.137  -0.235  -0.292  -0.375  -0.403  -0.378  -0.335  -0.241  -0.171  -0.252  -0.319  -0.366  -0.387  -0.370  -0.323  -0.229\n",
      "                         :   var12:  +0.164  +0.229  +0.339  +0.382  +0.399  +0.385  +0.338  +0.264  +0.121  +0.217  +0.307  +0.374  +1.000  +0.378  +0.342  +0.262  +0.072  +0.126  +0.215  +0.294  +0.357  +0.339  +0.268  +0.204  -0.011  -0.007  +0.037  +0.091  +0.174  +0.171  +0.136  +0.112  -0.152  -0.170  -0.187  -0.168  -0.120  -0.076  -0.022  +0.034  -0.214  -0.273  -0.336  -0.336  -0.316  -0.219  -0.154  -0.116  -0.264  -0.329  -0.372  -0.414  -0.391  -0.315  -0.233  -0.163  -0.254  -0.323  -0.379  -0.378  -0.393  -0.323  -0.239  -0.167\n",
      "                         :   var13:  +0.044  +0.093  +0.211  +0.279  +0.363  +0.374  +0.375  +0.285  +0.019  +0.074  +0.177  +0.287  +0.378  +1.000  +0.411  +0.351  -0.073  -0.010  +0.078  +0.205  +0.356  +0.387  +0.395  +0.312  -0.154  -0.137  -0.092  +0.043  +0.183  +0.275  +0.279  +0.255  -0.261  -0.293  -0.285  -0.181  -0.066  +0.051  +0.127  +0.163  -0.319  -0.354  -0.378  -0.349  -0.254  -0.105  -0.011  +0.031  -0.304  -0.385  -0.404  -0.392  -0.299  -0.194  -0.126  -0.037  -0.290  -0.363  -0.367  -0.366  -0.319  -0.235  -0.139  -0.080\n",
      "                         :   var14:  -0.000  +0.008  +0.111  +0.216  +0.309  +0.356  +0.370  +0.313  -0.079  -0.014  +0.067  +0.191  +0.342  +0.411  +1.000  +0.367  -0.136  -0.107  -0.026  +0.131  +0.307  +0.382  +0.408  +0.373  -0.220  -0.214  -0.181  -0.011  +0.190  +0.289  +0.349  +0.308  -0.325  -0.331  -0.308  -0.200  -0.010  +0.134  +0.211  +0.222  -0.319  -0.366  -0.363  -0.301  -0.172  -0.017  +0.089  +0.130  -0.312  -0.354  -0.393  -0.332  -0.220  -0.109  -0.024  +0.039  -0.255  -0.306  -0.334  -0.298  -0.235  -0.151  -0.065  -0.009\n",
      "                         :   var15:  -0.080  -0.037  +0.042  +0.131  +0.250  +0.298  +0.338  +0.290  -0.130  -0.075  -0.010  +0.123  +0.262  +0.351  +0.367  +1.000  -0.179  -0.173  -0.090  +0.054  +0.225  +0.356  +0.394  +0.336  -0.249  -0.261  -0.188  -0.041  +0.160  +0.294  +0.324  +0.334  -0.296  -0.331  -0.291  -0.175  -0.003  +0.164  +0.218  +0.259  -0.304  -0.340  -0.329  -0.261  -0.105  +0.056  +0.139  +0.178  -0.258  -0.320  -0.320  -0.253  -0.161  -0.042  +0.032  +0.076  -0.192  -0.254  -0.277  -0.221  -0.166  -0.075  -0.014  +0.018\n",
      "                         :   var16:  +0.281  +0.303  +0.292  +0.198  +0.069  -0.024  -0.085  -0.098  +0.344  +0.365  +0.321  +0.214  +0.072  -0.073  -0.136  -0.179  +1.000  +0.398  +0.344  +0.212  +0.008  -0.139  -0.234  -0.253  +0.384  +0.370  +0.308  +0.133  -0.092  -0.244  -0.294  -0.307  +0.315  +0.319  +0.203  +0.038  -0.173  -0.326  -0.371  -0.338  +0.244  +0.211  +0.125  -0.053  -0.228  -0.345  -0.372  -0.339  +0.153  +0.095  +0.043  -0.115  -0.229  -0.310  -0.339  -0.288  +0.062  +0.041  -0.026  -0.114  -0.203  -0.253  -0.279  -0.235\n",
      "                         :   var17:  +0.302  +0.340  +0.330  +0.254  +0.128  +0.047  -0.051  -0.067  +0.363  +0.400  +0.378  +0.281  +0.126  -0.010  -0.107  -0.173  +0.398  +1.000  +0.386  +0.265  +0.077  -0.126  -0.233  -0.243  +0.374  +0.384  +0.336  +0.155  -0.048  -0.249  -0.290  -0.297  +0.288  +0.295  +0.197  +0.046  -0.188  -0.341  -0.375  -0.359  +0.201  +0.179  +0.067  -0.093  -0.266  -0.394  -0.398  -0.376  +0.121  +0.067  -0.015  -0.152  -0.280  -0.357  -0.370  -0.331  +0.052  -0.013  -0.071  -0.164  -0.275  -0.299  -0.334  -0.258\n",
      "                         :   var18:  +0.311  +0.325  +0.352  +0.313  +0.216  +0.120  +0.018  -0.004  +0.344  +0.399  +0.397  +0.334  +0.215  +0.078  -0.026  -0.090  +0.344  +0.386  +1.000  +0.314  +0.138  -0.033  -0.127  -0.151  +0.326  +0.334  +0.321  +0.161  -0.001  -0.160  -0.240  -0.242  +0.203  +0.205  +0.139  -0.006  -0.192  -0.306  -0.342  -0.312  +0.115  +0.092  -0.025  -0.147  -0.317  -0.379  -0.390  -0.337  +0.028  -0.021  -0.111  -0.229  -0.320  -0.370  -0.369  -0.311  -0.040  -0.085  -0.138  -0.246  -0.317  -0.344  -0.343  -0.277\n",
      "                         :   var19:  +0.223  +0.288  +0.323  +0.331  +0.287  +0.223  +0.173  +0.082  +0.230  +0.321  +0.367  +0.329  +0.294  +0.205  +0.131  +0.054  +0.212  +0.265  +0.314  +1.000  +0.248  +0.140  +0.046  +0.004  +0.134  +0.179  +0.176  +0.114  +0.087  -0.016  -0.048  -0.088  +0.038  +0.039  -0.011  -0.063  -0.149  -0.208  -0.185  -0.164  -0.028  -0.081  -0.171  -0.215  -0.292  -0.319  -0.282  -0.222  -0.096  -0.170  -0.220  -0.290  -0.348  -0.350  -0.318  -0.222  -0.118  -0.198  -0.258  -0.312  -0.348  -0.333  -0.298  -0.239\n",
      "                         :   var20:  +0.099  +0.158  +0.245  +0.300  +0.344  +0.323  +0.298  +0.224  +0.088  +0.142  +0.231  +0.293  +0.357  +0.356  +0.307  +0.225  +0.008  +0.077  +0.138  +0.248  +1.000  +0.303  +0.268  +0.215  -0.061  -0.057  +0.022  +0.090  +0.154  +0.172  +0.158  +0.150  -0.174  -0.176  -0.196  -0.145  -0.066  -0.014  +0.008  +0.051  -0.223  -0.265  -0.312  -0.292  -0.260  -0.162  -0.113  -0.048  -0.251  -0.317  -0.354  -0.351  -0.321  -0.237  -0.178  -0.094  -0.220  -0.298  -0.323  -0.335  -0.329  -0.266  -0.193  -0.143\n",
      "                         :   var21:  -0.038  +0.017  +0.115  +0.209  +0.302  +0.344  +0.344  +0.290  -0.073  -0.009  +0.075  +0.195  +0.339  +0.387  +0.382  +0.356  -0.139  -0.126  -0.033  +0.140  +0.303  +1.000  +0.399  +0.360  -0.238  -0.228  -0.168  -0.006  +0.154  +0.301  +0.321  +0.292  -0.305  -0.328  -0.316  -0.196  -0.020  +0.127  +0.192  +0.224  -0.321  -0.356  -0.367  -0.306  -0.156  +0.005  +0.075  +0.124  -0.317  -0.355  -0.369  -0.311  -0.224  -0.096  -0.028  +0.029  -0.268  -0.310  -0.340  -0.297  -0.236  -0.146  -0.057  -0.022\n",
      "                         :   var22:  -0.120  -0.080  +0.035  +0.120  +0.254  +0.300  +0.347  +0.315  -0.159  -0.117  -0.019  +0.122  +0.268  +0.395  +0.408  +0.394  -0.234  -0.233  -0.127  +0.046  +0.268  +0.399  +1.000  +0.396  -0.328  -0.305  -0.247  -0.037  +0.183  +0.340  +0.394  +0.389  -0.371  -0.392  -0.350  -0.198  +0.028  +0.225  +0.317  +0.311  -0.389  -0.397  -0.393  -0.280  -0.083  +0.093  +0.193  +0.231  -0.340  -0.367  -0.360  -0.272  -0.127  -0.015  +0.079  +0.124  -0.259  -0.309  -0.314  -0.239  -0.171  -0.080  +0.035  +0.060\n",
      "                         :   var23:  -0.112  -0.106  -0.028  +0.066  +0.184  +0.261  +0.309  +0.271  -0.181  -0.157  -0.072  +0.053  +0.204  +0.312  +0.373  +0.336  -0.253  -0.243  -0.151  +0.004  +0.215  +0.360  +0.396  +1.000  -0.325  -0.318  -0.277  -0.091  +0.163  +0.323  +0.375  +0.374  -0.377  -0.361  -0.322  -0.174  +0.046  +0.235  +0.301  +0.333  -0.352  -0.372  -0.335  -0.213  -0.042  +0.130  +0.239  +0.254  -0.298  -0.317  -0.302  -0.221  -0.077  +0.060  +0.125  +0.154  -0.224  -0.253  -0.264  -0.189  -0.081  -0.013  +0.072  +0.093\n",
      "                         :   var24:  +0.254  +0.258  +0.218  +0.123  -0.012  -0.098  -0.177  -0.165  +0.328  +0.331  +0.269  +0.152  -0.011  -0.154  -0.220  -0.249  +0.384  +0.374  +0.326  +0.134  -0.061  -0.238  -0.328  -0.325  +1.000  +0.417  +0.316  +0.143  -0.130  -0.308  -0.367  -0.355  +0.386  +0.365  +0.286  +0.091  -0.154  -0.319  -0.390  -0.374  +0.309  +0.302  +0.193  +0.031  -0.174  -0.317  -0.365  -0.358  +0.233  +0.214  +0.143  -0.013  -0.168  -0.265  -0.344  -0.274  +0.119  +0.126  +0.076  -0.035  -0.138  -0.208  -0.242  -0.209\n",
      "                         :   var25:  +0.256  +0.269  +0.213  +0.118  +0.016  -0.102  -0.165  -0.177  +0.336  +0.341  +0.295  +0.159  -0.007  -0.137  -0.214  -0.261  +0.370  +0.384  +0.334  +0.179  -0.057  -0.228  -0.305  -0.318  +0.417  +1.000  +0.350  +0.161  -0.103  -0.321  -0.396  -0.362  +0.378  +0.362  +0.309  +0.095  -0.171  -0.347  -0.403  -0.399  +0.331  +0.292  +0.196  +0.035  -0.180  -0.349  -0.401  -0.386  +0.231  +0.199  +0.134  -0.016  -0.175  -0.283  -0.342  -0.315  +0.123  +0.115  +0.063  -0.052  -0.163  -0.235  -0.280  -0.247\n",
      "                         :   var26:  +0.243  +0.238  +0.213  +0.144  +0.041  -0.047  -0.107  -0.107  +0.283  +0.303  +0.263  +0.179  +0.037  -0.092  -0.181  -0.188  +0.308  +0.336  +0.321  +0.176  +0.022  -0.168  -0.247  -0.277  +0.316  +0.350  +1.000  +0.140  -0.077  -0.241  -0.303  -0.301  +0.307  +0.294  +0.250  +0.085  -0.149  -0.307  -0.350  -0.359  +0.226  +0.222  +0.113  +0.018  -0.195  -0.304  -0.350  -0.344  +0.168  +0.117  +0.070  -0.047  -0.163  -0.288  -0.319  -0.302  +0.091  +0.068  -0.005  -0.078  -0.168  -0.224  -0.282  -0.212\n",
      "                         :   var27:  +0.126  +0.154  +0.136  +0.136  +0.099  +0.061  +0.001  +0.003  +0.162  +0.180  +0.178  +0.163  +0.091  +0.043  -0.011  -0.041  +0.133  +0.155  +0.161  +0.114  +0.090  -0.006  -0.037  -0.091  +0.143  +0.161  +0.140  +1.000  +0.026  -0.090  -0.098  -0.135  +0.081  +0.094  +0.045  -0.005  -0.088  -0.139  -0.148  -0.158  +0.035  +0.021  -0.003  -0.072  -0.154  -0.170  -0.192  -0.178  -0.014  -0.002  -0.061  -0.109  -0.175  -0.205  -0.220  -0.162  -0.024  -0.091  -0.082  -0.141  -0.173  -0.172  -0.186  -0.139\n",
      "                         :   var28:  -0.008  +0.015  +0.069  +0.106  +0.135  +0.171  +0.178  +0.131  -0.046  +0.006  +0.059  +0.098  +0.174  +0.183  +0.190  +0.160  -0.092  -0.048  -0.001  +0.087  +0.154  +0.154  +0.183  +0.163  -0.130  -0.103  -0.077  +0.026  +1.000  +0.150  +0.172  +0.134  -0.172  -0.177  -0.151  -0.084  +0.010  +0.055  +0.099  +0.102  -0.191  -0.198  -0.217  -0.136  -0.097  -0.010  +0.060  +0.062  -0.184  -0.197  -0.201  -0.159  -0.119  -0.064  -0.032  -0.008  -0.150  -0.195  -0.197  -0.172  -0.131  -0.089  -0.059  -0.034\n",
      "                         :   var29:  -0.129  -0.116  -0.053  +0.031  +0.141  +0.210  +0.236  +0.226  -0.180  -0.161  -0.088  +0.034  +0.171  +0.275  +0.289  +0.294  -0.244  -0.249  -0.160  -0.016  +0.172  +0.301  +0.340  +0.323  -0.308  -0.321  -0.241  -0.090  +0.150  +1.000  +0.335  +0.320  -0.339  -0.354  -0.316  -0.165  +0.074  +0.248  +0.287  +0.305  -0.338  -0.339  -0.306  -0.171  -0.029  +0.160  +0.205  +0.244  -0.285  -0.311  -0.286  -0.182  -0.034  +0.077  +0.144  +0.154  -0.223  -0.245  -0.233  -0.139  -0.070  +0.017  +0.087  +0.103\n",
      "                         :   var30:  -0.168  -0.166  -0.105  +0.001  +0.134  +0.207  +0.263  +0.254  -0.238  -0.218  -0.163  -0.002  +0.136  +0.279  +0.349  +0.324  -0.294  -0.290  -0.240  -0.048  +0.158  +0.321  +0.394  +0.375  -0.367  -0.396  -0.303  -0.098  +0.172  +0.335  +1.000  +0.419  -0.389  -0.407  -0.334  -0.156  +0.101  +0.296  +0.365  +0.358  -0.387  -0.372  -0.323  -0.184  +0.031  +0.218  +0.302  +0.317  -0.307  -0.306  -0.289  -0.183  -0.020  +0.123  +0.201  +0.212  -0.252  -0.262  -0.232  -0.140  -0.033  +0.054  +0.138  +0.131\n",
      "                         :   var31:  -0.184  -0.186  -0.112  -0.036  +0.096  +0.192  +0.244  +0.243  -0.247  -0.236  -0.167  -0.033  +0.112  +0.255  +0.308  +0.334  -0.307  -0.297  -0.242  -0.088  +0.150  +0.292  +0.389  +0.374  -0.355  -0.362  -0.301  -0.135  +0.134  +0.320  +0.419  +1.000  -0.399  -0.386  -0.344  -0.171  +0.087  +0.290  +0.367  +0.381  -0.352  -0.353  -0.304  -0.178  +0.025  +0.224  +0.322  +0.322  -0.299  -0.291  -0.272  -0.135  +0.004  +0.154  +0.235  +0.223  -0.202  -0.230  -0.207  -0.108  -0.009  +0.088  +0.151  +0.160\n",
      "                         :   var32:  +0.158  +0.154  +0.097  -0.016  -0.128  -0.202  -0.240  -0.225  +0.256  +0.207  +0.143  +0.005  -0.152  -0.261  -0.325  -0.296  +0.315  +0.288  +0.203  +0.038  -0.174  -0.305  -0.371  -0.377  +0.386  +0.378  +0.307  +0.081  -0.172  -0.339  -0.389  -0.399  +1.000  +0.410  +0.336  +0.152  -0.117  -0.315  -0.381  -0.373  +0.390  +0.384  +0.306  +0.133  -0.077  -0.251  -0.331  -0.324  +0.334  +0.321  +0.288  +0.134  -0.040  -0.176  -0.248  -0.257  +0.241  +0.261  +0.217  +0.100  +0.011  -0.105  -0.189  -0.182\n",
      "                         :   var33:  +0.138  +0.123  +0.075  -0.034  -0.156  -0.208  -0.260  -0.259  +0.207  +0.205  +0.115  -0.005  -0.170  -0.293  -0.331  -0.331  +0.319  +0.295  +0.205  +0.039  -0.176  -0.328  -0.392  -0.361  +0.365  +0.362  +0.294  +0.094  -0.177  -0.354  -0.407  -0.386  +0.410  +1.000  +0.345  +0.172  -0.100  -0.294  -0.362  -0.371  +0.385  +0.375  +0.312  +0.185  -0.059  -0.251  -0.326  -0.316  +0.321  +0.317  +0.283  +0.151  -0.023  -0.175  -0.233  -0.239  +0.229  +0.257  +0.209  +0.095  +0.001  -0.096  -0.192  -0.171\n",
      "                         :   var34:  +0.081  +0.066  +0.010  -0.075  -0.147  -0.222  -0.246  -0.233  +0.150  +0.118  +0.058  -0.057  -0.187  -0.285  -0.308  -0.291  +0.203  +0.197  +0.139  -0.011  -0.196  -0.316  -0.350  -0.322  +0.286  +0.309  +0.250  +0.045  -0.151  -0.316  -0.334  -0.344  +0.336  +0.345  +1.000  +0.170  -0.083  -0.242  -0.316  -0.304  +0.351  +0.343  +0.326  +0.187  -0.001  -0.175  -0.234  -0.257  +0.298  +0.305  +0.272  +0.175  +0.035  -0.107  -0.173  -0.200  +0.244  +0.242  +0.235  +0.134  +0.068  -0.052  -0.121  -0.114\n",
      "                         :   var35:  -0.024  -0.045  -0.082  -0.109  -0.153  -0.171  -0.166  -0.141  +0.001  -0.001  -0.084  -0.115  -0.168  -0.181  -0.200  -0.175  +0.038  +0.046  -0.006  -0.063  -0.145  -0.196  -0.198  -0.174  +0.091  +0.095  +0.085  -0.005  -0.084  -0.165  -0.156  -0.171  +0.152  +0.172  +0.170  +1.000  -0.008  -0.079  -0.111  -0.141  +0.156  +0.157  +0.187  +0.144  +0.086  -0.021  -0.045  -0.088  +0.147  +0.183  +0.175  +0.170  +0.093  +0.022  -0.007  -0.043  +0.116  +0.145  +0.145  +0.143  +0.077  +0.052  +0.020  -0.009\n",
      "                         :   var36:  -0.160  -0.189  -0.197  -0.146  -0.133  -0.090  -0.056  -0.046  -0.176  -0.236  -0.193  -0.162  -0.120  -0.066  -0.010  -0.003  -0.173  -0.188  -0.192  -0.149  -0.066  -0.020  +0.028  +0.046  -0.154  -0.171  -0.149  -0.088  +0.010  +0.074  +0.101  +0.087  -0.117  -0.100  -0.083  -0.008  +1.000  +0.162  +0.157  +0.140  -0.080  -0.058  +0.008  +0.093  +0.153  +0.186  +0.187  +0.155  -0.061  +0.011  +0.034  +0.106  +0.164  +0.177  +0.188  +0.140  -0.015  -0.001  +0.068  +0.109  +0.167  +0.161  +0.154  +0.125\n",
      "                         :   var37:  -0.222  -0.248  -0.258  -0.167  -0.097  -0.001  +0.063  +0.094  -0.294  -0.321  -0.286  -0.191  -0.076  +0.051  +0.134  +0.164  -0.326  -0.341  -0.306  -0.208  -0.014  +0.127  +0.225  +0.235  -0.319  -0.347  -0.307  -0.139  +0.055  +0.248  +0.296  +0.290  -0.315  -0.294  -0.242  -0.079  +0.162  +1.000  +0.330  +0.323  -0.236  -0.229  -0.148  +0.014  +0.181  +0.321  +0.356  +0.331  -0.193  -0.153  -0.081  +0.037  +0.163  +0.286  +0.303  +0.271  -0.122  -0.095  -0.047  +0.045  +0.155  +0.216  +0.251  +0.210\n",
      "                         :   var38:  -0.260  -0.273  -0.219  -0.148  -0.032  +0.055  +0.124  +0.142  -0.308  -0.333  -0.294  -0.177  -0.022  +0.127  +0.211  +0.218  -0.371  -0.375  -0.342  -0.185  +0.008  +0.192  +0.317  +0.301  -0.390  -0.403  -0.350  -0.148  +0.099  +0.287  +0.365  +0.367  -0.381  -0.362  -0.316  -0.111  +0.157  +0.330  +1.000  +0.409  -0.320  -0.305  -0.211  -0.046  +0.173  +0.353  +0.397  +0.389  -0.240  -0.206  -0.144  +0.007  +0.174  +0.280  +0.352  +0.326  -0.157  -0.148  -0.115  +0.017  +0.124  +0.229  +0.282  +0.238\n",
      "                         :   var39:  -0.223  -0.224  -0.189  -0.114  -0.007  +0.093  +0.152  +0.178  -0.285  -0.299  -0.263  -0.138  +0.034  +0.163  +0.222  +0.259  -0.338  -0.359  -0.312  -0.164  +0.051  +0.224  +0.311  +0.333  -0.374  -0.399  -0.359  -0.158  +0.102  +0.305  +0.358  +0.381  -0.373  -0.371  -0.304  -0.141  +0.140  +0.323  +0.409  +1.000  -0.313  -0.307  -0.256  -0.084  +0.138  +0.319  +0.378  +0.368  -0.244  -0.226  -0.186  -0.055  +0.120  +0.270  +0.313  +0.288  -0.159  -0.157  -0.128  -0.023  +0.106  +0.201  +0.249  +0.218\n",
      "                         :   var40:  +0.093  +0.070  -0.009  -0.095  -0.185  -0.249  -0.272  -0.240  +0.173  +0.114  +0.046  -0.087  -0.214  -0.319  -0.319  -0.304  +0.244  +0.201  +0.115  -0.028  -0.223  -0.321  -0.389  -0.352  +0.309  +0.331  +0.226  +0.035  -0.191  -0.338  -0.387  -0.352  +0.390  +0.385  +0.351  +0.156  -0.080  -0.236  -0.320  -0.313  +1.000  +0.387  +0.354  +0.204  +0.010  -0.157  -0.247  -0.263  +0.356  +0.349  +0.327  +0.208  +0.055  -0.081  -0.170  -0.182  +0.280  +0.325  +0.281  +0.173  +0.069  -0.022  -0.106  -0.116\n",
      "                         :   var41:  +0.040  +0.013  -0.067  -0.145  -0.233  -0.296  -0.313  -0.267  +0.128  +0.055  -0.012  -0.143  -0.273  -0.354  -0.366  -0.340  +0.211  +0.179  +0.092  -0.081  -0.265  -0.356  -0.397  -0.372  +0.302  +0.292  +0.222  +0.021  -0.198  -0.339  -0.372  -0.353  +0.384  +0.375  +0.343  +0.157  -0.058  -0.229  -0.305  -0.307  +0.387  +1.000  +0.384  +0.255  +0.061  -0.127  -0.228  -0.229  +0.368  +0.387  +0.384  +0.267  +0.107  -0.046  -0.113  -0.165  +0.305  +0.338  +0.323  +0.243  +0.131  +0.025  -0.075  -0.090\n",
      "                         :   var42:  -0.028  -0.066  -0.157  -0.243  -0.317  -0.328  -0.316  -0.277  +0.028  -0.024  -0.130  -0.230  -0.336  -0.378  -0.363  -0.329  +0.125  +0.067  -0.025  -0.171  -0.312  -0.367  -0.393  -0.335  +0.193  +0.196  +0.113  -0.003  -0.217  -0.306  -0.323  -0.304  +0.306  +0.312  +0.326  +0.187  +0.008  -0.148  -0.211  -0.256  +0.354  +0.384  +1.000  +0.300  +0.151  -0.037  -0.111  -0.173  +0.344  +0.370  +0.403  +0.334  +0.214  +0.069  -0.000  -0.069  +0.302  +0.351  +0.371  +0.305  +0.227  +0.132  +0.026  -0.013\n",
      "                         :   var43:  -0.120  -0.185  -0.258  -0.285  -0.320  -0.314  -0.295  -0.239  -0.104  -0.165  -0.233  -0.295  -0.336  -0.349  -0.301  -0.261  -0.053  -0.093  -0.147  -0.215  -0.292  -0.306  -0.280  -0.213  +0.031  +0.035  +0.018  -0.072  -0.136  -0.171  -0.184  -0.178  +0.133  +0.185  +0.187  +0.144  +0.093  +0.014  -0.046  -0.084  +0.204  +0.255  +0.300  +1.000  +0.242  +0.111  +0.054  +0.013  +0.221  +0.293  +0.336  +0.343  +0.275  +0.208  +0.130  +0.045  +0.235  +0.272  +0.335  +0.316  +0.279  +0.216  +0.157  +0.083\n",
      "                         :   var44:  -0.226  -0.281  -0.341  -0.341  -0.313  -0.253  -0.191  -0.128  -0.259  -0.315  -0.345  -0.362  -0.316  -0.254  -0.172  -0.105  -0.228  -0.266  -0.317  -0.292  -0.260  -0.156  -0.083  -0.042  -0.174  -0.180  -0.195  -0.154  -0.097  -0.029  +0.031  +0.025  -0.077  -0.059  -0.001  +0.086  +0.153  +0.181  +0.173  +0.138  +0.010  +0.061  +0.151  +0.242  +1.000  +0.317  +0.283  +0.220  +0.054  +0.134  +0.212  +0.302  +0.340  +0.356  +0.310  +0.234  +0.107  +0.148  +0.239  +0.298  +0.350  +0.338  +0.305  +0.209\n",
      "                         :   var45:  -0.277  -0.309  -0.343  -0.307  -0.223  -0.159  -0.081  -0.030  -0.330  -0.391  -0.392  -0.318  -0.219  -0.105  -0.017  +0.056  -0.345  -0.394  -0.379  -0.319  -0.162  +0.005  +0.093  +0.130  -0.317  -0.349  -0.304  -0.170  -0.010  +0.160  +0.218  +0.224  -0.251  -0.251  -0.175  -0.021  +0.186  +0.321  +0.353  +0.319  -0.157  -0.127  -0.037  +0.111  +0.317  +1.000  +0.413  +0.347  -0.093  -0.014  +0.070  +0.202  +0.335  +0.408  +0.403  +0.335  -0.036  +0.027  +0.102  +0.192  +0.312  +0.349  +0.358  +0.290\n",
      "                         :   var46:  -0.276  -0.340  -0.311  -0.268  -0.168  -0.090  +0.012  +0.076  -0.346  -0.385  -0.382  -0.295  -0.154  -0.011  +0.089  +0.139  -0.372  -0.398  -0.390  -0.282  -0.113  +0.075  +0.193  +0.239  -0.365  -0.401  -0.350  -0.192  +0.060  +0.205  +0.302  +0.322  -0.331  -0.326  -0.234  -0.045  +0.187  +0.356  +0.397  +0.378  -0.247  -0.228  -0.111  +0.054  +0.283  +0.413  +1.000  +0.421  -0.161  -0.114  -0.042  +0.131  +0.283  +0.383  +0.422  +0.375  -0.088  -0.058  +0.020  +0.140  +0.272  +0.336  +0.356  +0.290\n",
      "                         :   var47:  -0.231  -0.284  -0.269  -0.209  -0.111  -0.013  +0.045  +0.102  -0.309  -0.336  -0.317  -0.227  -0.116  +0.031  +0.130  +0.178  -0.339  -0.376  -0.337  -0.222  -0.048  +0.124  +0.231  +0.254  -0.358  -0.386  -0.344  -0.178  +0.062  +0.244  +0.317  +0.322  -0.324  -0.316  -0.257  -0.088  +0.155  +0.331  +0.389  +0.368  -0.263  -0.229  -0.173  +0.013  +0.220  +0.347  +0.421  +1.000  -0.185  -0.147  -0.098  +0.066  +0.224  +0.337  +0.373  +0.329  -0.110  -0.091  -0.036  +0.080  +0.190  +0.261  +0.326  +0.257\n",
      "                         :   var48:  +0.011  -0.013  -0.078  -0.157  -0.218  -0.263  -0.248  -0.228  +0.075  +0.033  -0.037  -0.137  -0.264  -0.304  -0.312  -0.258  +0.153  +0.121  +0.028  -0.096  -0.251  -0.317  -0.340  -0.298  +0.233  +0.231  +0.168  -0.014  -0.184  -0.285  -0.307  -0.299  +0.334  +0.321  +0.298  +0.147  -0.061  -0.193  -0.240  -0.244  +0.356  +0.368  +0.344  +0.221  +0.054  -0.093  -0.161  -0.185  +1.000  +0.340  +0.345  +0.252  +0.102  -0.005  -0.086  -0.133  +0.265  +0.317  +0.305  +0.219  +0.133  +0.041  -0.049  -0.066\n",
      "                         :   var49:  -0.016  -0.067  -0.164  -0.221  -0.289  -0.323  -0.308  -0.259  +0.006  -0.045  -0.124  -0.235  -0.329  -0.385  -0.354  -0.320  +0.095  +0.067  -0.021  -0.170  -0.317  -0.355  -0.367  -0.317  +0.214  +0.199  +0.117  -0.002  -0.197  -0.311  -0.306  -0.291  +0.321  +0.317  +0.305  +0.183  +0.011  -0.153  -0.206  -0.226  +0.349  +0.387  +0.370  +0.293  +0.134  -0.014  -0.114  -0.147  +0.340  +1.000  +0.404  +0.349  +0.211  +0.069  -0.011  -0.068  +0.293  +0.369  +0.370  +0.314  +0.234  +0.122  +0.022  -0.015\n",
      "                         :   var50:  -0.072  -0.145  -0.228  -0.309  -0.337  -0.360  -0.331  -0.282  -0.057  -0.122  -0.206  -0.292  -0.372  -0.404  -0.393  -0.320  +0.043  -0.015  -0.111  -0.220  -0.354  -0.369  -0.360  -0.302  +0.143  +0.134  +0.070  -0.061  -0.201  -0.286  -0.289  -0.272  +0.288  +0.283  +0.272  +0.175  +0.034  -0.081  -0.144  -0.186  +0.327  +0.384  +0.403  +0.336  +0.212  +0.070  -0.042  -0.098  +0.345  +0.404  +1.000  +0.379  +0.278  +0.144  +0.072  -0.007  +0.303  +0.374  +0.401  +0.367  +0.289  +0.203  +0.098  +0.035\n",
      "                         :   var51:  -0.181  -0.236  -0.327  -0.377  -0.386  -0.373  -0.306  -0.254  -0.177  -0.248  -0.324  -0.375  -0.414  -0.392  -0.332  -0.253  -0.115  -0.152  -0.229  -0.290  -0.351  -0.311  -0.272  -0.221  -0.013  -0.016  -0.047  -0.109  -0.159  -0.182  -0.183  -0.135  +0.134  +0.151  +0.175  +0.170  +0.106  +0.037  +0.007  -0.055  +0.208  +0.267  +0.334  +0.343  +0.302  +0.202  +0.131  +0.066  +0.252  +0.349  +0.379  +1.000  +0.371  +0.277  +0.228  +0.120  +0.251  +0.354  +0.402  +0.409  +0.371  +0.303  +0.217  +0.139\n",
      "                         :   var52:  -0.243  -0.307  -0.382  -0.388  -0.355  -0.299  -0.230  -0.169  -0.254  -0.336  -0.395  -0.403  -0.391  -0.299  -0.220  -0.161  -0.229  -0.280  -0.320  -0.348  -0.321  -0.224  -0.127  -0.077  -0.168  -0.175  -0.163  -0.175  -0.119  -0.034  -0.020  +0.004  -0.040  -0.023  +0.035  +0.093  +0.164  +0.163  +0.174  +0.120  +0.055  +0.107  +0.214  +0.275  +0.340  +0.335  +0.283  +0.224  +0.102  +0.211  +0.278  +0.371  +1.000  +0.397  +0.345  +0.242  +0.154  +0.225  +0.316  +0.357  +0.422  +0.385  +0.325  +0.270\n",
      "                         :   var53:  -0.265  -0.329  -0.360  -0.360  -0.306  -0.222  -0.147  -0.101  -0.320  -0.381  -0.398  -0.378  -0.315  -0.194  -0.109  -0.042  -0.310  -0.357  -0.370  -0.350  -0.237  -0.096  -0.015  +0.060  -0.265  -0.283  -0.288  -0.205  -0.064  +0.077  +0.123  +0.154  -0.176  -0.175  -0.107  +0.022  +0.177  +0.286  +0.280  +0.270  -0.081  -0.046  +0.069  +0.208  +0.356  +0.408  +0.383  +0.337  -0.005  +0.069  +0.144  +0.277  +0.397  +1.000  +0.413  +0.338  +0.033  +0.116  +0.203  +0.299  +0.393  +0.403  +0.389  +0.326\n",
      "                         :   var54:  -0.263  -0.322  -0.340  -0.310  -0.238  -0.158  -0.086  -0.038  -0.317  -0.399  -0.383  -0.335  -0.233  -0.126  -0.024  +0.032  -0.339  -0.370  -0.369  -0.318  -0.178  -0.028  +0.079  +0.125  -0.344  -0.342  -0.319  -0.220  -0.032  +0.144  +0.201  +0.235  -0.248  -0.233  -0.173  -0.007  +0.188  +0.303  +0.352  +0.313  -0.170  -0.113  -0.000  +0.130  +0.310  +0.403  +0.422  +0.373  -0.086  -0.011  +0.072  +0.228  +0.345  +0.413  +1.000  +0.363  -0.014  +0.039  +0.115  +0.225  +0.346  +0.377  +0.379  +0.324\n",
      "                         :   var55:  -0.220  -0.268  -0.267  -0.221  -0.165  -0.077  -0.012  +0.013  -0.259  -0.308  -0.308  -0.241  -0.163  -0.037  +0.039  +0.076  -0.288  -0.331  -0.311  -0.222  -0.094  +0.029  +0.124  +0.154  -0.274  -0.315  -0.302  -0.162  -0.008  +0.154  +0.212  +0.223  -0.257  -0.239  -0.200  -0.043  +0.140  +0.271  +0.326  +0.288  -0.182  -0.165  -0.069  +0.045  +0.234  +0.335  +0.375  +0.329  -0.133  -0.068  -0.007  +0.120  +0.242  +0.338  +0.363  +1.000  -0.065  -0.019  +0.045  +0.133  +0.244  +0.276  +0.330  +0.239\n",
      "                         :   var56:  -0.015  -0.053  -0.123  -0.179  -0.207  -0.226  -0.217  -0.192  +0.005  -0.032  -0.092  -0.171  -0.254  -0.290  -0.255  -0.192  +0.062  +0.052  -0.040  -0.118  -0.220  -0.268  -0.259  -0.224  +0.119  +0.123  +0.091  -0.024  -0.150  -0.223  -0.252  -0.202  +0.241  +0.229  +0.244  +0.116  -0.015  -0.122  -0.157  -0.159  +0.280  +0.305  +0.302  +0.235  +0.107  -0.036  -0.088  -0.110  +0.265  +0.293  +0.303  +0.251  +0.154  +0.033  -0.014  -0.065  +1.000  +0.287  +0.294  +0.232  +0.173  +0.104  -0.009  -0.010\n",
      "                         :   var57:  -0.087  -0.123  -0.200  -0.255  -0.281  -0.284  -0.270  -0.219  -0.036  -0.087  -0.183  -0.252  -0.323  -0.363  -0.306  -0.254  +0.041  -0.013  -0.085  -0.198  -0.298  -0.310  -0.309  -0.253  +0.126  +0.115  +0.068  -0.091  -0.195  -0.245  -0.262  -0.230  +0.261  +0.257  +0.242  +0.145  -0.001  -0.095  -0.148  -0.157  +0.325  +0.338  +0.351  +0.272  +0.148  +0.027  -0.058  -0.091  +0.317  +0.369  +0.374  +0.354  +0.225  +0.116  +0.039  -0.019  +0.287  +1.000  +0.362  +0.314  +0.262  +0.152  +0.069  +0.034\n",
      "                         :   var58:  -0.123  -0.166  -0.252  -0.317  -0.347  -0.349  -0.315  -0.257  -0.086  -0.159  -0.243  -0.319  -0.379  -0.367  -0.334  -0.277  -0.026  -0.071  -0.138  -0.258  -0.323  -0.340  -0.314  -0.264  +0.076  +0.063  -0.005  -0.082  -0.197  -0.233  -0.232  -0.207  +0.217  +0.209  +0.235  +0.145  +0.068  -0.047  -0.115  -0.128  +0.281  +0.323  +0.371  +0.335  +0.239  +0.102  +0.020  -0.036  +0.305  +0.370  +0.401  +0.402  +0.316  +0.203  +0.115  +0.045  +0.294  +0.362  +1.000  +0.383  +0.324  +0.232  +0.142  +0.084\n",
      "                         :   var59:  -0.168  -0.231  -0.330  -0.359  -0.357  -0.314  -0.292  -0.215  -0.187  -0.249  -0.315  -0.366  -0.378  -0.366  -0.298  -0.221  -0.114  -0.164  -0.246  -0.312  -0.335  -0.297  -0.239  -0.189  -0.035  -0.052  -0.078  -0.141  -0.172  -0.139  -0.140  -0.108  +0.100  +0.095  +0.134  +0.143  +0.109  +0.045  +0.017  -0.023  +0.173  +0.243  +0.305  +0.316  +0.298  +0.192  +0.140  +0.080  +0.219  +0.314  +0.367  +0.409  +0.357  +0.299  +0.225  +0.133  +0.232  +0.314  +0.383  +1.000  +0.365  +0.329  +0.227  +0.158\n",
      "                         :   var60:  -0.228  -0.295  -0.363  -0.378  -0.361  -0.320  -0.254  -0.182  -0.250  -0.319  -0.378  -0.387  -0.393  -0.319  -0.235  -0.166  -0.203  -0.275  -0.317  -0.348  -0.329  -0.236  -0.171  -0.081  -0.138  -0.163  -0.168  -0.173  -0.131  -0.070  -0.033  -0.009  +0.011  +0.001  +0.068  +0.077  +0.167  +0.155  +0.124  +0.106  +0.069  +0.131  +0.227  +0.279  +0.350  +0.312  +0.272  +0.190  +0.133  +0.234  +0.289  +0.371  +0.422  +0.393  +0.346  +0.244  +0.173  +0.262  +0.324  +0.365  +1.000  +0.393  +0.326  +0.243\n",
      "                         :   var61:  -0.231  -0.284  -0.333  -0.340  -0.318  -0.253  -0.190  -0.105  -0.271  -0.342  -0.379  -0.370  -0.323  -0.235  -0.151  -0.075  -0.253  -0.299  -0.344  -0.333  -0.266  -0.146  -0.080  -0.013  -0.208  -0.235  -0.224  -0.172  -0.089  +0.017  +0.054  +0.088  -0.105  -0.096  -0.052  +0.052  +0.161  +0.216  +0.229  +0.201  -0.022  +0.025  +0.132  +0.216  +0.338  +0.349  +0.336  +0.261  +0.041  +0.122  +0.203  +0.303  +0.385  +0.403  +0.377  +0.276  +0.104  +0.152  +0.232  +0.329  +0.393  +1.000  +0.361  +0.272\n",
      "                         :   var62:  -0.225  -0.262  -0.313  -0.298  -0.241  -0.173  -0.101  -0.062  -0.280  -0.334  -0.345  -0.323  -0.239  -0.139  -0.065  -0.014  -0.279  -0.334  -0.343  -0.298  -0.193  -0.057  +0.035  +0.072  -0.242  -0.280  -0.282  -0.186  -0.059  +0.087  +0.138  +0.151  -0.189  -0.192  -0.121  +0.020  +0.154  +0.251  +0.282  +0.249  -0.106  -0.075  +0.026  +0.157  +0.305  +0.358  +0.356  +0.326  -0.049  +0.022  +0.098  +0.217  +0.325  +0.389  +0.379  +0.330  -0.009  +0.069  +0.142  +0.227  +0.326  +0.361  +1.000  +0.289\n",
      "                         :   var63:  -0.170  -0.217  -0.257  -0.210  -0.177  -0.105  -0.061  -0.002  -0.227  -0.254  -0.261  -0.229  -0.167  -0.080  -0.009  +0.018  -0.235  -0.258  -0.277  -0.239  -0.143  -0.022  +0.060  +0.093  -0.209  -0.247  -0.212  -0.139  -0.034  +0.103  +0.131  +0.160  -0.182  -0.171  -0.114  -0.009  +0.125  +0.210  +0.238  +0.218  -0.116  -0.090  -0.013  +0.083  +0.209  +0.290  +0.290  +0.257  -0.066  -0.015  +0.035  +0.139  +0.270  +0.326  +0.324  +0.239  -0.010  +0.034  +0.084  +0.158  +0.243  +0.272  +0.289  +1.000\n",
      "                         : ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "DataSetFactory           : [dataset] :  \n",
      "                         : \n"
     ]
    }
   ],
   "source": [
    "\n",
    "//Boosted Decision Trees\n",
    "factory.BookMethod(loader,TMVA::Types::kBDT, \"BDT\",\n",
    "                   \"!V:NTrees=800:MinNodeSize=2.5%:MaxDepth=2:BoostType=AdaBoost:AdaBoostBeta=0.5:UseBaggedBoost:BaggedSampleFraction=0.5:SeparationType=GiniIndex:nCuts=20\" );\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Booking Deep Neural Network\n",
    "\n",
    "Here we book the new DNN of TMVA. If using master version you can use the new DL method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool useDNN = true; \n",
    "bool useCNN = true; \n",
    "bool useKeras = false; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factory                  : Booking method: \u001b[1mDL_DENSE\u001b[0m\n",
      "                         : \n",
      "                         : Parsing option string: \n",
      "                         : ... \"!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIERUNIFORM:InputLayout=1|1|64:BatchLayout=1|128|64:Layout=DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|1|LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.9,Repetitions=1,ConvergenceSteps=20,BatchSize=128,TestRepetitions=1,MaxEpochs=20,WeightDecay=1e-4,Regularization=L2,Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0.:Architecture=Standard\"\n",
      "                         : The following options are set:\n",
      "                         : - By User:\n",
      "                         :     <none>\n",
      "                         : - Default:\n",
      "                         :     Boost_num: \"0\" [Number of times the classifier will be boosted]\n",
      "                         : Parsing option string: \n",
      "                         : ... \"!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIERUNIFORM:InputLayout=1|1|64:BatchLayout=1|128|64:Layout=DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|1|LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.9,Repetitions=1,ConvergenceSteps=20,BatchSize=128,TestRepetitions=1,MaxEpochs=20,WeightDecay=1e-4,Regularization=L2,Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0.:Architecture=Standard\"\n",
      "                         : The following options are set:\n",
      "                         : - By User:\n",
      "                         :     V: \"True\" [Verbose output (short form of \"VerbosityLevel\" below - overrides the latter one)]\n",
      "                         :     VarTransform: \"None\" [List of variable transformations performed before training, e.g., \"D_Background,P_Signal,G,N_AllClasses\" for: \"Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed)\"]\n",
      "                         :     H: \"False\" [Print method-specific help message]\n",
      "                         :     InputLayout: \"1|1|64\" [The Layout of the input]\n",
      "                         :     BatchLayout: \"1|128|64\" [The Layout of the batch]\n",
      "                         :     Layout: \"DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|1|LINEAR\" [Layout of the network.]\n",
      "                         :     ErrorStrategy: \"CROSSENTROPY\" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]\n",
      "                         :     WeightInitialization: \"XAVIERUNIFORM\" [Weight initialization strategy]\n",
      "                         :     Architecture: \"STANDARD\" [Which architecture to perform the training on.]\n",
      "                         :     TrainingStrategy: \"LearningRate=1e-3,Momentum=0.9,Repetitions=1,ConvergenceSteps=20,BatchSize=128,TestRepetitions=1,MaxEpochs=20,WeightDecay=1e-4,Regularization=L2,Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0.\" [Defines the training strategies.]\n",
      "                         : - Default:\n",
      "                         :     VerbosityLevel: \"Default\" [Verbosity level]\n",
      "                         :     CreateMVAPdfs: \"False\" [Create PDFs for classifier outputs (signal and background)]\n",
      "                         :     IgnoreNegWeightsInTraining: \"False\" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]\n",
      "                         :     RandomSeed: \"0\" [Random seed used for weight initialization and batch shuffling]\n",
      "                         : The STANDARD architecture has been deprecated. Please use Architecture=CPU or Architecture=CPU.See the TMVA Users' Guide for instructions if you encounter problems.\n",
      "                         : Will use the deprecated STANDARD architecture !\n"
     ]
    }
   ],
   "source": [
    "if (useDNN) { \n",
    "    \n",
    "     TString inputLayoutString = \"InputLayout=1|1|64\"; \n",
    "     TString batchLayoutString= \"BatchLayout=1|128|64\";\n",
    "     TString layoutString (\"Layout=DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|1|LINEAR\");\n",
    "//                                                                                                                                                                                       \n",
    "      // Training strategies \n",
    "      // one can catenate several training strategies \n",
    "      TString training1(\"LearningRate=1e-3,Momentum=0.9,Repetitions=1,\"\n",
    "                        \"ConvergenceSteps=20,BatchSize=128,TestRepetitions=1,\"\n",
    "                        \"MaxEpochs=20,WeightDecay=1e-4,Regularization=L2,\"\n",
    "                        \"Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0.\");\n",
    "  \n",
    "      TString trainingStrategyString (\"TrainingStrategy=\");\n",
    "      trainingStrategyString += training1; // + \"|\" + training2 + \"|\" + training3;\n",
    "\n",
    "      // General Options.                                                                                                                                                                \n",
    "      TString dnnOptions (\"!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:\"\n",
    "                          \"WeightInitialization=XAVIERUNIFORM\");\n",
    "      dnnOptions.Append (\":\"); dnnOptions.Append (inputLayoutString);\n",
    "      dnnOptions.Append (\":\"); dnnOptions.Append (batchLayoutString);\n",
    "      dnnOptions.Append (\":\"); dnnOptions.Append (layoutString);\n",
    "      dnnOptions.Append (\":\"); dnnOptions.Append (trainingStrategyString);\n",
    "\n",
    "      dnnOptions += \":Architecture=Standard\";\n",
    "      factory.BookMethod(loader, TMVA::Types::kDL, \"DL_DENSE\", dnnOptions);\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Book Convolutional Neural Network in TMVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factory                  : Booking method: \u001b[1mDL_CNN\u001b[0m\n",
      "                         : \n",
      "                         : Parsing option string: \n",
      "                         : ... \"!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIERUNIFORM:InputLayout=1|8|8:BatchLayout=128|1|64:Layout=CONV|10|3|3|1|1|1|1|RELU,CONV|10|3|3|1|1|1|1|RELU,MAXPOOL|2|2|1|1,RESHAPE|FLAT,DENSE|64|TANH,DENSE|1|LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.9,Repetitions=1,ConvergenceSteps=20,BatchSize=128,TestRepetitions=1,MaxEpochs=20,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0.0:Architecture=CPU\"\n",
      "                         : The following options are set:\n",
      "                         : - By User:\n",
      "                         :     <none>\n",
      "                         : - Default:\n",
      "                         :     Boost_num: \"0\" [Number of times the classifier will be boosted]\n",
      "                         : Parsing option string: \n",
      "                         : ... \"!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIERUNIFORM:InputLayout=1|8|8:BatchLayout=128|1|64:Layout=CONV|10|3|3|1|1|1|1|RELU,CONV|10|3|3|1|1|1|1|RELU,MAXPOOL|2|2|1|1,RESHAPE|FLAT,DENSE|64|TANH,DENSE|1|LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.9,Repetitions=1,ConvergenceSteps=20,BatchSize=128,TestRepetitions=1,MaxEpochs=20,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0.0:Architecture=CPU\"\n",
      "                         : The following options are set:\n",
      "                         : - By User:\n",
      "                         :     V: \"True\" [Verbose output (short form of \"VerbosityLevel\" below - overrides the latter one)]\n",
      "                         :     VarTransform: \"None\" [List of variable transformations performed before training, e.g., \"D_Background,P_Signal,G,N_AllClasses\" for: \"Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed)\"]\n",
      "                         :     H: \"False\" [Print method-specific help message]\n",
      "                         :     InputLayout: \"1|8|8\" [The Layout of the input]\n",
      "                         :     BatchLayout: \"128|1|64\" [The Layout of the batch]\n",
      "                         :     Layout: \"CONV|10|3|3|1|1|1|1|RELU,CONV|10|3|3|1|1|1|1|RELU,MAXPOOL|2|2|1|1,RESHAPE|FLAT,DENSE|64|TANH,DENSE|1|LINEAR\" [Layout of the network.]\n",
      "                         :     ErrorStrategy: \"CROSSENTROPY\" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]\n",
      "                         :     WeightInitialization: \"XAVIERUNIFORM\" [Weight initialization strategy]\n",
      "                         :     Architecture: \"CPU\" [Which architecture to perform the training on.]\n",
      "                         :     TrainingStrategy: \"LearningRate=1e-3,Momentum=0.9,Repetitions=1,ConvergenceSteps=20,BatchSize=128,TestRepetitions=1,MaxEpochs=20,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0.0\" [Defines the training strategies.]\n",
      "                         : - Default:\n",
      "                         :     VerbosityLevel: \"Default\" [Verbosity level]\n",
      "                         :     CreateMVAPdfs: \"False\" [Create PDFs for classifier outputs (signal and background)]\n",
      "                         :     IgnoreNegWeightsInTraining: \"False\" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]\n",
      "                         :     RandomSeed: \"0\" [Random seed used for weight initialization and batch shuffling]\n",
      "                         : Will use now the CPU architecture !\n"
     ]
    }
   ],
   "source": [
    "if (useCNN) { \n",
    "    TString inputLayoutString(\"InputLayout=1|8|8\");\n",
    "                                                                                                \n",
    "// Batch Layout                                                                                                                                     \n",
    "    TString batchLayoutString(\"BatchLayout=128|1|64\");\n",
    "                                                   \n",
    "\n",
    "TString layoutString(\"Layout=CONV|10|3|3|1|1|1|1|RELU,CONV|10|3|3|1|1|1|1|RELU,MAXPOOL|2|2|1|1,\"\n",
    "                     \"RESHAPE|FLAT,DENSE|64|TANH,DENSE|1|LINEAR\");\n",
    "                                                                                                                                              \n",
    "\n",
    "\n",
    "   // Training strategies.                                                                                                                          \n",
    "   TString training0(\"LearningRate=1e-3,Momentum=0.9,Repetitions=1,\"\n",
    "                     \"ConvergenceSteps=20,BatchSize=128,TestRepetitions=1,\"\n",
    "                     \"MaxEpochs=20,WeightDecay=1e-4,Regularization=None,\"\n",
    "                     \"Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0.0\");\n",
    " \n",
    "   TString trainingStrategyString (\"TrainingStrategy=\");\n",
    "   trainingStrategyString += training0; // + \"|\" + training1 + \"|\" + training2;   }\n",
    "    \n",
    "// General Options.                                                                                                                              \n",
    "   TString cnnOptions (\"!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:\"\n",
    "                       \"WeightInitialization=XAVIERUNIFORM\");\n",
    "\n",
    "   cnnOptions.Append(\":\"); cnnOptions.Append(inputLayoutString);\n",
    "   cnnOptions.Append(\":\"); cnnOptions.Append(batchLayoutString);\n",
    "   cnnOptions.Append(\":\"); cnnOptions.Append(layoutString);\n",
    "   cnnOptions.Append(\":\"); cnnOptions.Append(trainingStrategyString);\n",
    "   cnnOptions.Append(\":Architecture=CPU\");\n",
    "\n",
    "   //// New DL (CNN)                                                                                                                                \n",
    "\n",
    "\n",
    "  factory.BookMethod(loader, TMVA::Types::kDL, \"DL_CNN\", cnnOptions);\n",
    "\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Book Convolutional Neural Network in Keras using a generated model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (useKeras) { \n",
    "   factory.BookMethod(loader, TMVA::Types::kPyKeras, \n",
    "                       \"PyKeras\",\"H:!V:VarTransform=None:FilenameModel=model_cnn.h5:\"\n",
    "                       \"FilenameTrainedModel=trained_model_cnn.h5:NumEpochs=20:BatchSize=256\");\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factory                  : \u001b[1mTrain all methods\u001b[0m\n",
      "Factory                  : [dataset] : Create Transformation \"I\" with events from all classes.\n",
      "                         : \n",
      "                         : Transformation, Variable selection : \n",
      "                         : Input : variable 'var0' <---> Output : variable 'var0'\n",
      "                         : Input : variable 'var1' <---> Output : variable 'var1'\n",
      "                         : Input : variable 'var2' <---> Output : variable 'var2'\n",
      "                         : Input : variable 'var3' <---> Output : variable 'var3'\n",
      "                         : Input : variable 'var4' <---> Output : variable 'var4'\n",
      "                         : Input : variable 'var5' <---> Output : variable 'var5'\n",
      "                         : Input : variable 'var6' <---> Output : variable 'var6'\n",
      "                         : Input : variable 'var7' <---> Output : variable 'var7'\n",
      "                         : Input : variable 'var8' <---> Output : variable 'var8'\n",
      "                         : Input : variable 'var9' <---> Output : variable 'var9'\n",
      "                         : Input : variable 'var10' <---> Output : variable 'var10'\n",
      "                         : Input : variable 'var11' <---> Output : variable 'var11'\n",
      "                         : Input : variable 'var12' <---> Output : variable 'var12'\n",
      "                         : Input : variable 'var13' <---> Output : variable 'var13'\n",
      "                         : Input : variable 'var14' <---> Output : variable 'var14'\n",
      "                         : Input : variable 'var15' <---> Output : variable 'var15'\n",
      "                         : Input : variable 'var16' <---> Output : variable 'var16'\n",
      "                         : Input : variable 'var17' <---> Output : variable 'var17'\n",
      "                         : Input : variable 'var18' <---> Output : variable 'var18'\n",
      "                         : Input : variable 'var19' <---> Output : variable 'var19'\n",
      "                         : Input : variable 'var20' <---> Output : variable 'var20'\n",
      "                         : Input : variable 'var21' <---> Output : variable 'var21'\n",
      "                         : Input : variable 'var22' <---> Output : variable 'var22'\n",
      "                         : Input : variable 'var23' <---> Output : variable 'var23'\n",
      "                         : Input : variable 'var24' <---> Output : variable 'var24'\n",
      "                         : Input : variable 'var25' <---> Output : variable 'var25'\n",
      "                         : Input : variable 'var26' <---> Output : variable 'var26'\n",
      "                         : Input : variable 'var27' <---> Output : variable 'var27'\n",
      "                         : Input : variable 'var28' <---> Output : variable 'var28'\n",
      "                         : Input : variable 'var29' <---> Output : variable 'var29'\n",
      "                         : Input : variable 'var30' <---> Output : variable 'var30'\n",
      "                         : Input : variable 'var31' <---> Output : variable 'var31'\n",
      "                         : Input : variable 'var32' <---> Output : variable 'var32'\n",
      "                         : Input : variable 'var33' <---> Output : variable 'var33'\n",
      "                         : Input : variable 'var34' <---> Output : variable 'var34'\n",
      "                         : Input : variable 'var35' <---> Output : variable 'var35'\n",
      "                         : Input : variable 'var36' <---> Output : variable 'var36'\n",
      "                         : Input : variable 'var37' <---> Output : variable 'var37'\n",
      "                         : Input : variable 'var38' <---> Output : variable 'var38'\n",
      "                         : Input : variable 'var39' <---> Output : variable 'var39'\n",
      "                         : Input : variable 'var40' <---> Output : variable 'var40'\n",
      "                         : Input : variable 'var41' <---> Output : variable 'var41'\n",
      "                         : Input : variable 'var42' <---> Output : variable 'var42'\n",
      "                         : Input : variable 'var43' <---> Output : variable 'var43'\n",
      "                         : Input : variable 'var44' <---> Output : variable 'var44'\n",
      "                         : Input : variable 'var45' <---> Output : variable 'var45'\n",
      "                         : Input : variable 'var46' <---> Output : variable 'var46'\n",
      "                         : Input : variable 'var47' <---> Output : variable 'var47'\n",
      "                         : Input : variable 'var48' <---> Output : variable 'var48'\n",
      "                         : Input : variable 'var49' <---> Output : variable 'var49'\n",
      "                         : Input : variable 'var50' <---> Output : variable 'var50'\n",
      "                         : Input : variable 'var51' <---> Output : variable 'var51'\n",
      "                         : Input : variable 'var52' <---> Output : variable 'var52'\n",
      "                         : Input : variable 'var53' <---> Output : variable 'var53'\n",
      "                         : Input : variable 'var54' <---> Output : variable 'var54'\n",
      "                         : Input : variable 'var55' <---> Output : variable 'var55'\n",
      "                         : Input : variable 'var56' <---> Output : variable 'var56'\n",
      "                         : Input : variable 'var57' <---> Output : variable 'var57'\n",
      "                         : Input : variable 'var58' <---> Output : variable 'var58'\n",
      "                         : Input : variable 'var59' <---> Output : variable 'var59'\n",
      "                         : Input : variable 'var60' <---> Output : variable 'var60'\n",
      "                         : Input : variable 'var61' <---> Output : variable 'var61'\n",
      "                         : Input : variable 'var62' <---> Output : variable 'var62'\n",
      "                         : Input : variable 'var63' <---> Output : variable 'var63'\n",
      "TFHandler_Factory        : Variable        Mean        RMS   [        Min        Max ]\n",
      "                         : -----------------------------------------------------------\n",
      "                         :     var0:     2.4991     3.8926   [    -10.257     22.057 ]\n",
      "                         :     var1:     3.9787     4.5075   [    -10.122     25.016 ]\n",
      "                         :     var2:     5.5671     4.8848   [    -9.4795     26.928 ]\n",
      "                         :     var3:     6.5379     5.0192   [    -9.7689     29.230 ]\n",
      "                         :     var4:     6.3871     4.9931   [    -11.476     28.503 ]\n",
      "                         :     var5:     5.4269     4.8235   [    -9.1293     30.122 ]\n",
      "                         :     var6:     3.9107     4.4655   [    -11.141     25.933 ]\n",
      "                         :     var7:     2.4388     3.8766   [    -9.8326     22.021 ]\n",
      "                         :     var8:     3.7375     4.4169   [    -9.0364     24.243 ]\n",
      "                         :     var9:     6.0120     5.1559   [    -9.2157     28.235 ]\n",
      "                         :    var10:     8.3558     5.4951   [    -7.3822     29.694 ]\n",
      "                         :    var11:     9.8310     5.5008   [    -7.2496     32.294 ]\n",
      "                         :    var12:     9.7061     5.5019   [    -9.2380     31.239 ]\n",
      "                         :    var13:     8.2453     5.5157   [    -8.3455     31.442 ]\n",
      "                         :    var14:     5.9694     5.0886   [    -9.2506     29.027 ]\n",
      "                         :    var15:     3.6435     4.3955   [    -10.587     23.653 ]\n",
      "                         :    var16:     4.8733     4.7778   [    -11.709     25.905 ]\n",
      "                         :    var17:     7.9688     5.5580   [    -7.5348     33.189 ]\n",
      "                         :    var18:     10.951     5.7539   [    -7.3453     34.627 ]\n",
      "                         :    var19:     12.853     5.4921   [    -4.1117     33.786 ]\n",
      "                         :    var20:     12.820     5.4867   [    -5.3808     32.315 ]\n",
      "                         :    var21:     10.889     5.7651   [    -6.3743     36.072 ]\n",
      "                         :    var22:     7.9053     5.5376   [    -7.9329     32.559 ]\n",
      "                         :    var23:     4.7962     4.7362   [    -10.919     23.587 ]\n",
      "                         :    var24:     5.6555     5.0342   [    -9.8326     29.659 ]\n",
      "                         :    var25:     9.1977     5.6658   [    -6.3310     34.142 ]\n",
      "                         :    var26:     12.554     5.6514   [    -5.9403     33.692 ]\n",
      "                         :    var27:     14.795     5.1093   [    -2.1615     37.430 ]\n",
      "                         :    var28:     14.660     5.2013   [    -2.8283     38.805 ]\n",
      "                         :    var29:     12.449     5.6721   [    -4.6051     37.288 ]\n",
      "                         :    var30:     9.0063     5.7010   [    -10.813     33.124 ]\n",
      "                         :    var31:     5.4970     4.9834   [    -10.336     30.010 ]\n",
      "                         :    var32:     5.6520     5.0125   [    -9.2211     27.428 ]\n",
      "                         :    var33:     9.1908     5.7259   [    -8.0987     33.206 ]\n",
      "                         :    var34:     12.684     5.6427   [    -5.9139     36.323 ]\n",
      "                         :    var35:     14.812     5.1435   [    -3.2133     34.607 ]\n",
      "                         :    var36:     14.707     5.1390   [    -2.8380     35.026 ]\n",
      "                         :    var37:     12.530     5.5950   [    -6.1929     33.746 ]\n",
      "                         :    var38:     9.0409     5.7810   [    -8.0031     32.626 ]\n",
      "                         :    var39:     5.5780     4.9512   [    -8.8630     29.784 ]\n",
      "                         :    var40:     4.9770     4.7758   [    -10.145     25.266 ]\n",
      "                         :    var41:     7.8714     5.5548   [    -8.5314     37.635 ]\n",
      "                         :    var42:     11.016     5.7577   [    -8.1448     36.124 ]\n",
      "                         :    var43:     12.821     5.4932   [    -6.6966     43.293 ]\n",
      "                         :    var44:     12.887     5.5346   [    -4.9059     34.631 ]\n",
      "                         :    var45:     10.988     5.8018   [    -6.8219     34.300 ]\n",
      "                         :    var46:     7.9529     5.5516   [    -10.776     32.909 ]\n",
      "                         :    var47:     4.8126     4.7994   [    -9.0429     27.598 ]\n",
      "                         :    var48:     3.8114     4.3913   [    -10.010     25.702 ]\n",
      "                         :    var49:     6.0972     5.0664   [    -8.8429     30.707 ]\n",
      "                         :    var50:     8.4053     5.4578   [    -8.4494     30.113 ]\n",
      "                         :    var51:     9.8185     5.5505   [    -8.6899     33.313 ]\n",
      "                         :    var52:     9.8044     5.5527   [    -6.3026     33.478 ]\n",
      "                         :    var53:     8.2980     5.4937   [    -8.6684     35.304 ]\n",
      "                         :    var54:     5.9857     5.0800   [    -8.5084     28.695 ]\n",
      "                         :    var55:     3.5967     4.2971   [    -8.7073     21.957 ]\n",
      "                         :    var56:     2.5031     3.9146   [    -11.497     20.361 ]\n",
      "                         :    var57:     3.9950     4.4431   [    -8.8077     25.394 ]\n",
      "                         :    var58:     5.6013     4.8508   [    -11.076     26.883 ]\n",
      "                         :    var59:     6.5296     5.0636   [    -9.0219     28.571 ]\n",
      "                         :    var60:     6.6129     5.0677   [    -9.9130     27.121 ]\n",
      "                         :    var61:     5.5612     4.9134   [    -9.6182     26.950 ]\n",
      "                         :    var62:     4.0187     4.5265   [    -11.569     26.781 ]\n",
      "                         :    var63:     2.4755     3.8720   [    -9.2495     22.465 ]\n",
      "                         : -----------------------------------------------------------\n",
      "                         : \u001b[32m\n",
      "                         : <PlotVariables> Will not produce scatter plots ==> \n",
      "                         : |  The number of 64 input variables and 0 target values would require 2016 two-dimensional\n",
      "                         : |  histograms, which would occupy the computer's memory. Note that this\n",
      "                         : |  suppression does not have any consequences for your analysis, other\n",
      "                         : |  than not disposing of these scatter plots. You can modify the maximum\n",
      "                         : |  number of input variables allowed to generate scatter plots in your\n",
      "                         : |  script via the command line:\n",
      "                         : |  \"(TMVA::gConfig().GetVariablePlotting()).fMaxNumOfAllowedVariablesForScatterPlots = <some int>;\"\u001b[0m\n",
      "                         : \n",
      "                         : Some more output\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         : Ranking input variables (method unspecific)...\n",
      "IdTransformation         : Ranking result (top variable is best ranked)\n",
      "                         : ------------------------------\n",
      "                         : Rank : Variable  : Separation\n",
      "                         : ------------------------------\n",
      "                         :    1 : var31     : 1.987e-02\n",
      "                         :    2 : var60     : 1.452e-02\n",
      "                         :    3 : var39     : 1.448e-02\n",
      "                         :    4 : var38     : 1.352e-02\n",
      "                         :    5 : var11     : 1.318e-02\n",
      "                         :    6 : var4      : 1.276e-02\n",
      "                         :    7 : var24     : 1.248e-02\n",
      "                         :    8 : var23     : 1.231e-02\n",
      "                         :    9 : var59     : 1.229e-02\n",
      "                         :   10 : var30     : 1.203e-02\n",
      "                         :   11 : var51     : 1.037e-02\n",
      "                         :   12 : var3      : 9.927e-03\n",
      "                         :   13 : var32     : 9.479e-03\n",
      "                         :   14 : var47     : 8.533e-03\n",
      "                         :   15 : var40     : 8.417e-03\n",
      "                         :   16 : var58     : 8.149e-03\n",
      "                         :   17 : var25     : 8.039e-03\n",
      "                         :   18 : var2      : 8.016e-03\n",
      "                         :   19 : var12     : 7.996e-03\n",
      "                         :   20 : var46     : 7.747e-03\n",
      "                         :   21 : var33     : 7.741e-03\n",
      "                         :   22 : var16     : 7.662e-03\n",
      "                         :   23 : var5      : 7.407e-03\n",
      "                         :   24 : var41     : 6.987e-03\n",
      "                         :   25 : var55     : 6.755e-03\n",
      "                         :   26 : var43     : 6.707e-03\n",
      "                         :   27 : var26     : 6.565e-03\n",
      "                         :   28 : var13     : 6.508e-03\n",
      "                         :   29 : var52     : 6.318e-03\n",
      "                         :   30 : var15     : 6.001e-03\n",
      "                         :   31 : var61     : 5.907e-03\n",
      "                         :   32 : var22     : 5.683e-03\n",
      "                         :   33 : var29     : 5.676e-03\n",
      "                         :   34 : var54     : 5.618e-03\n",
      "                         :   35 : var20     : 5.605e-03\n",
      "                         :   36 : var37     : 5.505e-03\n",
      "                         :   37 : var19     : 5.274e-03\n",
      "                         :   38 : var17     : 4.922e-03\n",
      "                         :   39 : var49     : 4.797e-03\n",
      "                         :   40 : var8      : 4.496e-03\n",
      "                         :   41 : var7      : 4.491e-03\n",
      "                         :   42 : var34     : 4.484e-03\n",
      "                         :   43 : var48     : 4.478e-03\n",
      "                         :   44 : var9      : 4.435e-03\n",
      "                         :   45 : var50     : 4.398e-03\n",
      "                         :   46 : var18     : 4.369e-03\n",
      "                         :   47 : var53     : 4.348e-03\n",
      "                         :   48 : var27     : 4.313e-03\n",
      "                         :   49 : var57     : 4.163e-03\n",
      "                         :   50 : var10     : 4.135e-03\n",
      "                         :   51 : var45     : 4.055e-03\n",
      "                         :   52 : var35     : 3.998e-03\n",
      "                         :   53 : var1      : 3.670e-03\n",
      "                         :   54 : var44     : 3.477e-03\n",
      "                         :   55 : var14     : 3.376e-03\n",
      "                         :   56 : var56     : 3.157e-03\n",
      "                         :   57 : var6      : 3.078e-03\n",
      "                         :   58 : var62     : 2.999e-03\n",
      "                         :   59 : var21     : 2.938e-03\n",
      "                         :   60 : var36     : 2.937e-03\n",
      "                         :   61 : var42     : 2.906e-03\n",
      "                         :   62 : var28     : 2.700e-03\n",
      "                         :   63 : var63     : 2.451e-03\n",
      "                         :   64 : var0      : 2.151e-03\n",
      "                         : ------------------------------\n",
      "Factory                  : Train method: BDT for Classification\n",
      "                         : \n",
      "BDT                      : #events: (reweighted) sig: 5000 bkg: 5000\n",
      "                         : #events: (unweighted) sig: 5000 bkg: 5000\n",
      "                         : Training 800 Decision Trees ... patience please\n",
      "                         : Elapsed time for training with 10000 events: 27.9 sec         \n",
      "BDT                      : [dataset] : Evaluation of BDT on training sample (10000 events)\n",
      "                         : Elapsed time for evaluation of 10000 events: 0.785 sec       \n",
      "                         : Creating xml weight file: \u001b[0;36mdataset/weights/TMVA_CNN_Classification_BDT.weights.xml\u001b[0m\n",
      "                         : Creating standalone class: \u001b[0;36mdataset/weights/TMVA_CNN_Classification_BDT.class.C\u001b[0m\n",
      "Factory                  : Training finished\n",
      "                         : \n",
      "Factory                  : Train method: DL_DENSE for Classification\n",
      "                         : \n",
      "                         : Start of deep neural network training on the STANDARD architecture\n",
      "                         : \n",
      "                         : *****   Deep Learning Network *****\n",
      "DEEP NEURAL NETWORK:   Depth = 5  Input = ( 1, 1, 64 )  Batch size = 128  Loss function = C\n",
      "\tLayer 0\t DENSE Layer: \t  ( Input = 64 , Width = 64 ) \tOutput = ( 1 , 128 , 64 ) \t Activation Function = Tanh\n",
      "\tLayer 1\t DENSE Layer: \t  ( Input = 64 , Width = 64 ) \tOutput = ( 1 , 128 , 64 ) \t Activation Function = Tanh\n",
      "\tLayer 2\t DENSE Layer: \t  ( Input = 64 , Width = 64 ) \tOutput = ( 1 , 128 , 64 ) \t Activation Function = Tanh\n",
      "\tLayer 3\t DENSE Layer: \t  ( Input = 64 , Width = 64 ) \tOutput = ( 1 , 128 , 64 ) \t Activation Function = Tanh\n",
      "\tLayer 4\t DENSE Layer: \t  ( Input = 64 , Width = 1 ) \tOutput = ( 1 , 128 , 1 ) \t Activation Function = Identity\n",
      "                         : Training phase 1 of 1:    Learning rate = 0.001 regularization 2 minimum error = 0.747881\n",
      "                         : --------------------------------------------------------------\n",
      "                         :      Epoch |   Train Err.   Test Err.  t(s)/epoch   t(s)/Loss   nEvents/s Conv. Steps\n",
      "                         : --------------------------------------------------------------\n",
      "                         :          1 Minimum Test error found - save the configuration \n",
      "                         :          1 |     0.632517    0.652034     1.41221    0.645062     13014.5           0\n",
      "                         :          2 Minimum Test error found - save the configuration \n",
      "                         :          2 |     0.587504    0.626325     1.48479    0.747606     13543.4           0\n",
      "                         :          3 Minimum Test error found - save the configuration \n",
      "                         :          3 |     0.573191    0.618994       1.347    0.639646     14114.5           0\n",
      "                         :          4 Minimum Test error found - save the configuration \n",
      "                         :          4 |     0.561842    0.614024     1.34019    0.635113     14160.1           0\n",
      "                         :          5 Minimum Test error found - save the configuration \n",
      "                         :          5 |     0.554114    0.612078     1.34511    0.637961     14118.7           0\n",
      "                         :          6 Minimum Test error found - save the configuration \n",
      "                         :          6 |     0.546156    0.610024     1.34925    0.642824     14133.2           0\n",
      "                         :          7 Minimum Test error found - save the configuration \n",
      "                         :          7 |     0.540699     0.60871     1.41432    0.709982     14174.9           0\n",
      "                         :          8 |     0.541898    0.612854     1.45868     0.63074     12058.8           1\n",
      "                         :          9 Minimum Test error found - save the configuration \n",
      "                         :          9 |     0.532388    0.608617     1.33026    0.633355     14326.2           0\n",
      "                         :         10 |     0.530261    0.612226     1.32741    0.630206     14320.1           1\n",
      "                         :         11 |     0.526632    0.609379      1.3282    0.630594     14311.7           2\n",
      "                         :         12 |     0.521292    0.608911     1.32902    0.631139     14306.2           3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         :         13 |     0.517303    0.608678     1.32992    0.631272     14290.5           4\n",
      "                         :         14 |     0.517377    0.611123     1.33154      0.6319     14270.3           5\n",
      "                         :         15 |     0.512917    0.610003     1.33062    0.631887     14288.8           6\n",
      "                         :         16 |     0.507154    0.608932     1.33195    0.631084     14245.2           7\n",
      "                         :         17 |     0.508203    0.613312      1.3324    0.631237     14239.2           8\n",
      "                         :         18 Minimum Test error found - save the configuration \n",
      "                         :         18 |     0.502338    0.608333     1.34281    0.639731     14200.4           0\n",
      "                         :         19 |     0.501009    0.612067     1.32936    0.631743     14311.6           1\n",
      "                         :         20 |     0.496809    0.613487     1.32856    0.630886     14310.3           2\n",
      "                         : \n",
      "                         : Elapsed time for training with 10000 events: 27.6 sec         \n",
      "                         : Evaluate deep neural network on the STANDARD architecture  using batches with size = 128\n",
      "                         : \n",
      "DL_DENSE                 : [dataset] : Evaluation of DL_DENSE on training sample (10000 events)\n",
      "                         : Elapsed time for evaluation of 10000 events: 0.319 sec       \n",
      "                         : Creating xml weight file: \u001b[0;36mdataset/weights/TMVA_CNN_Classification_DL_DENSE.weights.xml\u001b[0m\n",
      "                         : Creating standalone class: \u001b[0;36mdataset/weights/TMVA_CNN_Classification_DL_DENSE.class.C\u001b[0m\n",
      "Factory                  : Training finished\n",
      "                         : \n",
      "Factory                  : Train method: DL_CNN for Classification\n",
      "                         : \n",
      "                         : Start of deep neural network training on CPU.\n",
      "                         : \n",
      "                         : *****   Deep Learning Network *****\n",
      "DEEP NEURAL NETWORK:   Depth = 6  Input = ( 1, 8, 8 )  Batch size = 128  Loss function = C\n",
      "\tLayer 0\t CONV LAYER: \t( W = 8 ,  H = 8 ,  D = 10 ) \t Filter ( W = 3 ,  H = 3 ) \tOutput = ( 128 , 10 , 64 ) \t Activation Function = Relu\n",
      "\tLayer 1\t CONV LAYER: \t( W = 8 ,  H = 8 ,  D = 10 ) \t Filter ( W = 3 ,  H = 3 ) \tOutput = ( 128 , 10 , 64 ) \t Activation Function = Relu\n",
      "\tLayer 2\t POOL Layer: \t( W = 7 ,  H = 7 ,  D = 10 ) \t Filter ( W = 2 ,  H = 2 ) \tOutput = ( 128 , 10 , 49 ) \n",
      "\tLayer 3\t RESHAPE Layer \t Input = ( 10 , 7 , 7 ) \tOutput = ( 1 , 128 , 490 ) \n",
      "\tLayer 4\t DENSE Layer: \t  ( Input = 490 , Width = 64 ) \tOutput = ( 1 , 128 , 64 ) \t Activation Function = Tanh\n",
      "\tLayer 5\t DENSE Layer: \t  ( Input = 64 , Width = 1 ) \tOutput = ( 1 , 128 , 1 ) \t Activation Function = Identity\n",
      "                         : Training phase 1 of 1:    Learning rate = 0.001 regularization 0 minimum error = 0.868508\n",
      "                         : --------------------------------------------------------------\n",
      "                         :      Epoch |   Train Err.   Test Err.  t(s)/epoch   t(s)/Loss   nEvents/s Conv. Steps\n",
      "                         : --------------------------------------------------------------\n",
      "                         :          1 Minimum Test error found - save the configuration \n",
      "                         :          1 |     0.580637    0.596397     15.1045     6.70039     1187.99           0\n",
      "                         :          2 Minimum Test error found - save the configuration \n",
      "                         :          2 |     0.550405    0.576383     14.7878     6.69962     1234.39           0\n",
      "                         :          3 Minimum Test error found - save the configuration \n",
      "                         :          3 |     0.536161    0.570594     14.7118     6.61302     1232.77           0\n",
      "                         :          4 Minimum Test error found - save the configuration \n",
      "                         :          4 |     0.526181    0.568024     14.5971     6.30096     1203.44           0\n",
      "                         :          5 Minimum Test error found - save the configuration \n",
      "                         :          5 |     0.515165    0.566359      13.995     6.39585     1313.83           0\n",
      "                         :          6 Minimum Test error found - save the configuration \n",
      "                         :          6 |     0.510672    0.564977     15.1964     6.79621     1188.55           0\n",
      "                         :          7 |     0.506073    0.567063     14.9095     7.10354     1279.03           1\n",
      "                         :          8 Minimum Test error found - save the configuration \n",
      "                         :          8 |     0.495796    0.563395     14.8003     6.70632     1233.51           0\n",
      "                         :          9 Minimum Test error found - save the configuration \n",
      "                         :          9 |      0.49215    0.561902     15.2013      7.0112     1219.03           0\n",
      "                         :         10 |     0.486683    0.562675     15.2011     6.61538     1162.86           1\n",
      "                         :         11 |      0.48351    0.563618     14.0939     5.60231     1175.75           2\n",
      "                         :         12 |     0.479481    0.563255     15.2931     7.18939     1232.03           3\n",
      "                         :         13 |     0.475133    0.563579     15.8101     7.00551     1133.96           4\n",
      "                         :         14 |      0.46989    0.563046     15.3898     6.99689     1189.58           5\n",
      "                         :         15 |     0.468917    0.565595     15.4011     6.70414     1147.99           6\n",
      "                         :         16 |     0.462434    0.563582     16.3965     7.39102     1108.65           7\n",
      "                         :         17 |     0.458846    0.565307     15.3095      6.6078     1147.37           8\n",
      "                         :         18 |     0.455704    0.566632     15.3004     7.10218     1217.83           9\n",
      "                         :         19 |     0.453148    0.568446      14.788     6.99167     1280.61          10\n",
      "                         :         20 |     0.447119    0.568914     14.7127     6.21339     1174.69          11\n",
      "                         : \n",
      "                         : Elapsed time for training with 10000 events: 308 sec         \n",
      "                         : Evaluate deep neural network on CPU using batches with size = 128\n",
      "                         : \n",
      "DL_CNN                   : [dataset] : Evaluation of DL_CNN on training sample (10000 events)\n",
      "                         : Elapsed time for evaluation of 10000 events: 3.21 sec       \n",
      "                         : Creating xml weight file: \u001b[0;36mdataset/weights/TMVA_CNN_Classification_DL_CNN.weights.xml\u001b[0m\n",
      "                         : Creating standalone class: \u001b[0;36mdataset/weights/TMVA_CNN_Classification_DL_CNN.class.C\u001b[0m\n",
      "Factory                  : Training finished\n",
      "                         : \n",
      "                         : Ranking input variables (method specific)...\n",
      "BDT                      : Ranking result (top variable is best ranked)\n",
      "                         : ---------------------------------------\n",
      "                         : Rank : Variable  : Variable Importance\n",
      "                         : ---------------------------------------\n",
      "                         :    1 : var4      : 2.133e-02\n",
      "                         :    2 : var31     : 2.026e-02\n",
      "                         :    3 : var32     : 2.000e-02\n",
      "                         :    4 : var51     : 1.973e-02\n",
      "                         :    5 : var39     : 1.958e-02\n",
      "                         :    6 : var11     : 1.937e-02\n",
      "                         :    7 : var60     : 1.902e-02\n",
      "                         :    8 : var24     : 1.844e-02\n",
      "                         :    9 : var12     : 1.828e-02\n",
      "                         :   10 : var29     : 1.808e-02\n",
      "                         :   11 : var17     : 1.800e-02\n",
      "                         :   12 : var3      : 1.798e-02\n",
      "                         :   13 : var13     : 1.766e-02\n",
      "                         :   14 : var59     : 1.760e-02\n",
      "                         :   15 : var58     : 1.755e-02\n",
      "                         :   16 : var2      : 1.739e-02\n",
      "                         :   17 : var23     : 1.720e-02\n",
      "                         :   18 : var40     : 1.710e-02\n",
      "                         :   19 : var53     : 1.702e-02\n",
      "                         :   20 : var30     : 1.697e-02\n",
      "                         :   21 : var49     : 1.677e-02\n",
      "                         :   22 : var15     : 1.654e-02\n",
      "                         :   23 : var25     : 1.654e-02\n",
      "                         :   24 : var41     : 1.636e-02\n",
      "                         :   25 : var37     : 1.635e-02\n",
      "                         :   26 : var16     : 1.602e-02\n",
      "                         :   27 : var20     : 1.593e-02\n",
      "                         :   28 : var26     : 1.590e-02\n",
      "                         :   29 : var22     : 1.575e-02\n",
      "                         :   30 : var47     : 1.561e-02\n",
      "                         :   31 : var54     : 1.550e-02\n",
      "                         :   32 : var46     : 1.550e-02\n",
      "                         :   33 : var38     : 1.530e-02\n",
      "                         :   34 : var43     : 1.515e-02\n",
      "                         :   35 : var8      : 1.509e-02\n",
      "                         :   36 : var33     : 1.504e-02\n",
      "                         :   37 : var19     : 1.503e-02\n",
      "                         :   38 : var5      : 1.499e-02\n",
      "                         :   39 : var45     : 1.488e-02\n",
      "                         :   40 : var21     : 1.472e-02\n",
      "                         :   41 : var10     : 1.466e-02\n",
      "                         :   42 : var56     : 1.449e-02\n",
      "                         :   43 : var61     : 1.443e-02\n",
      "                         :   44 : var48     : 1.439e-02\n",
      "                         :   45 : var62     : 1.397e-02\n",
      "                         :   46 : var50     : 1.394e-02\n",
      "                         :   47 : var27     : 1.385e-02\n",
      "                         :   48 : var52     : 1.385e-02\n",
      "                         :   49 : var35     : 1.374e-02\n",
      "                         :   50 : var42     : 1.368e-02\n",
      "                         :   51 : var9      : 1.363e-02\n",
      "                         :   52 : var7      : 1.348e-02\n",
      "                         :   53 : var6      : 1.339e-02\n",
      "                         :   54 : var57     : 1.325e-02\n",
      "                         :   55 : var55     : 1.315e-02\n",
      "                         :   56 : var36     : 1.313e-02\n",
      "                         :   57 : var28     : 1.302e-02\n",
      "                         :   58 : var63     : 1.276e-02\n",
      "                         :   59 : var18     : 1.248e-02\n",
      "                         :   60 : var14     : 1.237e-02\n",
      "                         :   61 : var44     : 1.205e-02\n",
      "                         :   62 : var1      : 1.201e-02\n",
      "                         :   63 : var0      : 1.178e-02\n",
      "                         :   64 : var34     : 1.095e-02\n",
      "                         : ---------------------------------------\n",
      "                         : No variable ranking supplied by classifier: DL_DENSE\n",
      "                         : No variable ranking supplied by classifier: DL_CNN\n",
      "Factory                  : === Destroy and recreate all methods via weight files for testing ===\n",
      "                         : \n",
      "                         : Reading weight file: \u001b[0;36mdataset/weights/TMVA_CNN_Classification_BDT.weights.xml\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         : Reading weight file: \u001b[0;36mdataset/weights/TMVA_CNN_Classification_DL_DENSE.weights.xml\u001b[0m\n",
      "                         : Reading weight file: \u001b[0;36mdataset/weights/TMVA_CNN_Classification_DL_CNN.weights.xml\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "factory.TrainAllMethods();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test and Evaluate Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factory                  : \u001b[1mTest all methods\u001b[0m\n",
      "Factory                  : Test method: BDT for Classification performance\n",
      "                         : \n",
      "BDT                      : [dataset] : Evaluation of BDT on testing sample (10000 events)\n",
      "                         : Elapsed time for evaluation of 10000 events: 0.527 sec       \n",
      "Factory                  : Test method: DL_DENSE for Classification performance\n",
      "                         : \n",
      "                         : Evaluate deep neural network on the STANDARD architecture  using batches with size = 1000\n",
      "                         : \n",
      "DL_DENSE                 : [dataset] : Evaluation of DL_DENSE on testing sample (10000 events)\n",
      "                         : Elapsed time for evaluation of 10000 events: 0.454 sec       \n",
      "Factory                  : Test method: DL_CNN for Classification performance\n",
      "                         : \n",
      "                         : Evaluate deep neural network on CPU using batches with size = 1000\n",
      "                         : \n",
      "DL_CNN                   : [dataset] : Evaluation of DL_CNN on testing sample (10000 events)\n",
      "                         : Elapsed time for evaluation of 10000 events: 1.43 sec       \n"
     ]
    }
   ],
   "source": [
    "factory.TestAllMethods();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factory                  : \u001b[1mEvaluate all methods\u001b[0m\n",
      "Factory                  : Evaluate classifier: BDT\n",
      "                         : \n",
      "BDT                      : [dataset] : Loop over test events and fill histograms with classifier response...\n",
      "                         : \n",
      "TFHandler_BDT            : Variable        Mean        RMS   [        Min        Max ]\n",
      "                         : -----------------------------------------------------------\n",
      "                         :     var0:     2.4375     3.8557   [    -9.9918     19.977 ]\n",
      "                         :     var1:     3.9666     4.4410   [    -9.7103     25.867 ]\n",
      "                         :     var2:     5.4753     4.8697   [    -8.5006     29.836 ]\n",
      "                         :     var3:     6.4672     5.0291   [    -7.9837     27.817 ]\n",
      "                         :     var4:     6.5185     5.0298   [    -10.282     27.355 ]\n",
      "                         :     var5:     5.4855     4.8555   [    -7.9858     27.156 ]\n",
      "                         :     var6:     3.9606     4.4945   [    -9.8938     28.512 ]\n",
      "                         :     var7:     2.4387     3.9116   [    -9.9916     21.240 ]\n",
      "                         :     var8:     3.6854     4.3626   [    -9.7199     27.343 ]\n",
      "                         :     var9:     6.0793     5.1200   [    -8.6642     29.457 ]\n",
      "                         :    var10:     8.3184     5.5430   [    -8.5650     31.127 ]\n",
      "                         :    var11:     9.6351     5.5072   [    -7.6700     32.336 ]\n",
      "                         :    var12:     9.6312     5.5191   [    -8.7419     29.683 ]\n",
      "                         :    var13:     8.3128     5.5239   [    -7.2193     36.270 ]\n",
      "                         :    var14:     6.0416     5.1057   [    -9.4517     30.755 ]\n",
      "                         :    var15:     3.6383     4.4067   [    -12.546     22.148 ]\n",
      "                         :    var16:     4.8401     4.7558   [    -11.279     26.243 ]\n",
      "                         :    var17:     7.9437     5.5305   [    -9.1076     32.552 ]\n",
      "                         :    var18:     10.885     5.7504   [    -11.120     35.900 ]\n",
      "                         :    var19:     12.854     5.5008   [    -5.4607     34.438 ]\n",
      "                         :    var20:     12.877     5.5496   [    -4.9795     34.453 ]\n",
      "                         :    var21:     10.946     5.7644   [    -6.3943     38.104 ]\n",
      "                         :    var22:     7.8424     5.4984   [    -7.1890     35.488 ]\n",
      "                         :    var23:     4.8228     4.7835   [    -9.2727     25.210 ]\n",
      "                         :    var24:     5.6459     4.9148   [    -8.6297     25.267 ]\n",
      "                         :    var25:     9.0667     5.6465   [    -7.8932     30.653 ]\n",
      "                         :    var26:     12.553     5.7546   [    -7.4565     38.462 ]\n",
      "                         :    var27:     14.670     5.1246   [    -1.2005     34.769 ]\n",
      "                         :    var28:     14.686     5.1706   [    -1.8391     35.169 ]\n",
      "                         :    var29:     12.466     5.7116   [    -6.3975     36.999 ]\n",
      "                         :    var30:     9.0021     5.7010   [    -8.3901     32.022 ]\n",
      "                         :    var31:     5.6036     5.0045   [    -9.6496     26.932 ]\n",
      "                         :    var32:     5.5822     4.9826   [    -11.560     26.953 ]\n",
      "                         :    var33:     9.1635     5.6928   [    -7.8908     31.500 ]\n",
      "                         :    var34:     12.670     5.6927   [    -8.4293     39.896 ]\n",
      "                         :    var35:     14.712     5.1167   [    -2.6615     34.365 ]\n",
      "                         :    var36:     14.777     5.1516   [    -2.1642     33.879 ]\n",
      "                         :    var37:     12.528     5.6390   [    -7.0200     33.836 ]\n",
      "                         :    var38:     9.0389     5.6715   [    -9.4451     32.219 ]\n",
      "                         :    var39:     5.5557     4.9419   [    -9.8782     27.489 ]\n",
      "                         :    var40:     4.9449     4.8088   [    -10.422     28.788 ]\n",
      "                         :    var41:     7.9424     5.5420   [    -8.4661     31.055 ]\n",
      "                         :    var42:     11.034     5.7483   [    -6.7219     34.394 ]\n",
      "                         :    var43:     12.846     5.4598   [    -6.0367     35.064 ]\n",
      "                         :    var44:     12.885     5.5471   [    -5.3399     36.017 ]\n",
      "                         :    var45:     11.017     5.7452   [    -8.1370     34.427 ]\n",
      "                         :    var46:     7.9332     5.5564   [    -9.6217     31.459 ]\n",
      "                         :    var47:     4.9462     4.8152   [    -9.0932     24.864 ]\n",
      "                         :    var48:     3.7166     4.3715   [    -10.844     25.228 ]\n",
      "                         :    var49:     6.1139     5.1265   [    -8.5402     29.845 ]\n",
      "                         :    var50:     8.3390     5.4683   [    -8.2915     32.722 ]\n",
      "                         :    var51:     9.8672     5.5727   [    -8.3201     33.142 ]\n",
      "                         :    var52:     9.8495     5.5322   [    -8.3466     33.869 ]\n",
      "                         :    var53:     8.4185     5.4828   [    -9.7019     33.503 ]\n",
      "                         :    var54:     6.0872     5.1308   [    -9.1295     32.443 ]\n",
      "                         :    var55:     3.7002     4.3962   [    -10.585     27.256 ]\n",
      "                         :    var56:     2.4642     3.9476   [    -9.7419     22.620 ]\n",
      "                         :    var57:     4.0422     4.5454   [    -8.2481     26.215 ]\n",
      "                         :    var58:     5.5688     4.8957   [    -7.9015     30.791 ]\n",
      "                         :    var59:     6.5518     5.0494   [    -8.9191     29.350 ]\n",
      "                         :    var60:     6.5388     5.0481   [    -8.4943     28.476 ]\n",
      "                         :    var61:     5.6626     4.9085   [    -9.0860     29.674 ]\n",
      "                         :    var62:     4.0584     4.5142   [    -9.4862     23.124 ]\n",
      "                         :    var63:     2.5276     3.9035   [    -10.527     21.548 ]\n",
      "                         : -----------------------------------------------------------\n",
      "                         : \u001b[32m\n",
      "                         : <PlotVariables> Will not produce scatter plots ==> \n",
      "                         : |  The number of 64 input variables and 0 target values would require 2016 two-dimensional\n",
      "                         : |  histograms, which would occupy the computer's memory. Note that this\n",
      "                         : |  suppression does not have any consequences for your analysis, other\n",
      "                         : |  than not disposing of these scatter plots. You can modify the maximum\n",
      "                         : |  number of input variables allowed to generate scatter plots in your\n",
      "                         : |  script via the command line:\n",
      "                         : |  \"(TMVA::gConfig().GetVariablePlotting()).fMaxNumOfAllowedVariablesForScatterPlots = <some int>;\"\u001b[0m\n",
      "                         : \n",
      "                         : Some more output\n",
      "Factory                  : Evaluate classifier: DL_DENSE\n",
      "                         : \n",
      "DL_DENSE                 : [dataset] : Loop over test events and fill histograms with classifier response...\n",
      "                         : \n",
      "                         : Evaluate deep neural network on the STANDARD architecture  using batches with size = 1000\n",
      "                         : \n",
      "TFHandler_DL_DENSE       : Variable        Mean        RMS   [        Min        Max ]\n",
      "                         : -----------------------------------------------------------\n",
      "                         :     var0:     2.4375     3.8557   [    -9.9918     19.977 ]\n",
      "                         :     var1:     3.9666     4.4410   [    -9.7103     25.867 ]\n",
      "                         :     var2:     5.4753     4.8697   [    -8.5006     29.836 ]\n",
      "                         :     var3:     6.4672     5.0291   [    -7.9837     27.817 ]\n",
      "                         :     var4:     6.5185     5.0298   [    -10.282     27.355 ]\n",
      "                         :     var5:     5.4855     4.8555   [    -7.9858     27.156 ]\n",
      "                         :     var6:     3.9606     4.4945   [    -9.8938     28.512 ]\n",
      "                         :     var7:     2.4387     3.9116   [    -9.9916     21.240 ]\n",
      "                         :     var8:     3.6854     4.3626   [    -9.7199     27.343 ]\n",
      "                         :     var9:     6.0793     5.1200   [    -8.6642     29.457 ]\n",
      "                         :    var10:     8.3184     5.5430   [    -8.5650     31.127 ]\n",
      "                         :    var11:     9.6351     5.5072   [    -7.6700     32.336 ]\n",
      "                         :    var12:     9.6312     5.5191   [    -8.7419     29.683 ]\n",
      "                         :    var13:     8.3128     5.5239   [    -7.2193     36.270 ]\n",
      "                         :    var14:     6.0416     5.1057   [    -9.4517     30.755 ]\n",
      "                         :    var15:     3.6383     4.4067   [    -12.546     22.148 ]\n",
      "                         :    var16:     4.8401     4.7558   [    -11.279     26.243 ]\n",
      "                         :    var17:     7.9437     5.5305   [    -9.1076     32.552 ]\n",
      "                         :    var18:     10.885     5.7504   [    -11.120     35.900 ]\n",
      "                         :    var19:     12.854     5.5008   [    -5.4607     34.438 ]\n",
      "                         :    var20:     12.877     5.5496   [    -4.9795     34.453 ]\n",
      "                         :    var21:     10.946     5.7644   [    -6.3943     38.104 ]\n",
      "                         :    var22:     7.8424     5.4984   [    -7.1890     35.488 ]\n",
      "                         :    var23:     4.8228     4.7835   [    -9.2727     25.210 ]\n",
      "                         :    var24:     5.6459     4.9148   [    -8.6297     25.267 ]\n",
      "                         :    var25:     9.0667     5.6465   [    -7.8932     30.653 ]\n",
      "                         :    var26:     12.553     5.7546   [    -7.4565     38.462 ]\n",
      "                         :    var27:     14.670     5.1246   [    -1.2005     34.769 ]\n",
      "                         :    var28:     14.686     5.1706   [    -1.8391     35.169 ]\n",
      "                         :    var29:     12.466     5.7116   [    -6.3975     36.999 ]\n",
      "                         :    var30:     9.0021     5.7010   [    -8.3901     32.022 ]\n",
      "                         :    var31:     5.6036     5.0045   [    -9.6496     26.932 ]\n",
      "                         :    var32:     5.5822     4.9826   [    -11.560     26.953 ]\n",
      "                         :    var33:     9.1635     5.6928   [    -7.8908     31.500 ]\n",
      "                         :    var34:     12.670     5.6927   [    -8.4293     39.896 ]\n",
      "                         :    var35:     14.712     5.1167   [    -2.6615     34.365 ]\n",
      "                         :    var36:     14.777     5.1516   [    -2.1642     33.879 ]\n",
      "                         :    var37:     12.528     5.6390   [    -7.0200     33.836 ]\n",
      "                         :    var38:     9.0389     5.6715   [    -9.4451     32.219 ]\n",
      "                         :    var39:     5.5557     4.9419   [    -9.8782     27.489 ]\n",
      "                         :    var40:     4.9449     4.8088   [    -10.422     28.788 ]\n",
      "                         :    var41:     7.9424     5.5420   [    -8.4661     31.055 ]\n",
      "                         :    var42:     11.034     5.7483   [    -6.7219     34.394 ]\n",
      "                         :    var43:     12.846     5.4598   [    -6.0367     35.064 ]\n",
      "                         :    var44:     12.885     5.5471   [    -5.3399     36.017 ]\n",
      "                         :    var45:     11.017     5.7452   [    -8.1370     34.427 ]\n",
      "                         :    var46:     7.9332     5.5564   [    -9.6217     31.459 ]\n",
      "                         :    var47:     4.9462     4.8152   [    -9.0932     24.864 ]\n",
      "                         :    var48:     3.7166     4.3715   [    -10.844     25.228 ]\n",
      "                         :    var49:     6.1139     5.1265   [    -8.5402     29.845 ]\n",
      "                         :    var50:     8.3390     5.4683   [    -8.2915     32.722 ]\n",
      "                         :    var51:     9.8672     5.5727   [    -8.3201     33.142 ]\n",
      "                         :    var52:     9.8495     5.5322   [    -8.3466     33.869 ]\n",
      "                         :    var53:     8.4185     5.4828   [    -9.7019     33.503 ]\n",
      "                         :    var54:     6.0872     5.1308   [    -9.1295     32.443 ]\n",
      "                         :    var55:     3.7002     4.3962   [    -10.585     27.256 ]\n",
      "                         :    var56:     2.4642     3.9476   [    -9.7419     22.620 ]\n",
      "                         :    var57:     4.0422     4.5454   [    -8.2481     26.215 ]\n",
      "                         :    var58:     5.5688     4.8957   [    -7.9015     30.791 ]\n",
      "                         :    var59:     6.5518     5.0494   [    -8.9191     29.350 ]\n",
      "                         :    var60:     6.5388     5.0481   [    -8.4943     28.476 ]\n",
      "                         :    var61:     5.6626     4.9085   [    -9.0860     29.674 ]\n",
      "                         :    var62:     4.0584     4.5142   [    -9.4862     23.124 ]\n",
      "                         :    var63:     2.5276     3.9035   [    -10.527     21.548 ]\n",
      "                         : -----------------------------------------------------------\n",
      "                         : \u001b[32m\n",
      "                         : <PlotVariables> Will not produce scatter plots ==> \n",
      "                         : |  The number of 64 input variables and 0 target values would require 2016 two-dimensional\n",
      "                         : |  histograms, which would occupy the computer's memory. Note that this\n",
      "                         : |  suppression does not have any consequences for your analysis, other\n",
      "                         : |  than not disposing of these scatter plots. You can modify the maximum\n",
      "                         : |  number of input variables allowed to generate scatter plots in your\n",
      "                         : |  script via the command line:\n",
      "                         : |  \"(TMVA::gConfig().GetVariablePlotting()).fMaxNumOfAllowedVariablesForScatterPlots = <some int>;\"\u001b[0m\n",
      "                         : \n",
      "                         : Some more output\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factory                  : Evaluate classifier: DL_CNN\n",
      "                         : \n",
      "DL_CNN                   : [dataset] : Loop over test events and fill histograms with classifier response...\n",
      "                         : \n",
      "                         : Evaluate deep neural network on CPU using batches with size = 1000\n",
      "                         : \n",
      "TFHandler_DL_CNN         : Variable        Mean        RMS   [        Min        Max ]\n",
      "                         : -----------------------------------------------------------\n",
      "                         :     var0:     2.4375     3.8557   [    -9.9918     19.977 ]\n",
      "                         :     var1:     3.9666     4.4410   [    -9.7103     25.867 ]\n",
      "                         :     var2:     5.4753     4.8697   [    -8.5006     29.836 ]\n",
      "                         :     var3:     6.4672     5.0291   [    -7.9837     27.817 ]\n",
      "                         :     var4:     6.5185     5.0298   [    -10.282     27.355 ]\n",
      "                         :     var5:     5.4855     4.8555   [    -7.9858     27.156 ]\n",
      "                         :     var6:     3.9606     4.4945   [    -9.8938     28.512 ]\n",
      "                         :     var7:     2.4387     3.9116   [    -9.9916     21.240 ]\n",
      "                         :     var8:     3.6854     4.3626   [    -9.7199     27.343 ]\n",
      "                         :     var9:     6.0793     5.1200   [    -8.6642     29.457 ]\n",
      "                         :    var10:     8.3184     5.5430   [    -8.5650     31.127 ]\n",
      "                         :    var11:     9.6351     5.5072   [    -7.6700     32.336 ]\n",
      "                         :    var12:     9.6312     5.5191   [    -8.7419     29.683 ]\n",
      "                         :    var13:     8.3128     5.5239   [    -7.2193     36.270 ]\n",
      "                         :    var14:     6.0416     5.1057   [    -9.4517     30.755 ]\n",
      "                         :    var15:     3.6383     4.4067   [    -12.546     22.148 ]\n",
      "                         :    var16:     4.8401     4.7558   [    -11.279     26.243 ]\n",
      "                         :    var17:     7.9437     5.5305   [    -9.1076     32.552 ]\n",
      "                         :    var18:     10.885     5.7504   [    -11.120     35.900 ]\n",
      "                         :    var19:     12.854     5.5008   [    -5.4607     34.438 ]\n",
      "                         :    var20:     12.877     5.5496   [    -4.9795     34.453 ]\n",
      "                         :    var21:     10.946     5.7644   [    -6.3943     38.104 ]\n",
      "                         :    var22:     7.8424     5.4984   [    -7.1890     35.488 ]\n",
      "                         :    var23:     4.8228     4.7835   [    -9.2727     25.210 ]\n",
      "                         :    var24:     5.6459     4.9148   [    -8.6297     25.267 ]\n",
      "                         :    var25:     9.0667     5.6465   [    -7.8932     30.653 ]\n",
      "                         :    var26:     12.553     5.7546   [    -7.4565     38.462 ]\n",
      "                         :    var27:     14.670     5.1246   [    -1.2005     34.769 ]\n",
      "                         :    var28:     14.686     5.1706   [    -1.8391     35.169 ]\n",
      "                         :    var29:     12.466     5.7116   [    -6.3975     36.999 ]\n",
      "                         :    var30:     9.0021     5.7010   [    -8.3901     32.022 ]\n",
      "                         :    var31:     5.6036     5.0045   [    -9.6496     26.932 ]\n",
      "                         :    var32:     5.5822     4.9826   [    -11.560     26.953 ]\n",
      "                         :    var33:     9.1635     5.6928   [    -7.8908     31.500 ]\n",
      "                         :    var34:     12.670     5.6927   [    -8.4293     39.896 ]\n",
      "                         :    var35:     14.712     5.1167   [    -2.6615     34.365 ]\n",
      "                         :    var36:     14.777     5.1516   [    -2.1642     33.879 ]\n",
      "                         :    var37:     12.528     5.6390   [    -7.0200     33.836 ]\n",
      "                         :    var38:     9.0389     5.6715   [    -9.4451     32.219 ]\n",
      "                         :    var39:     5.5557     4.9419   [    -9.8782     27.489 ]\n",
      "                         :    var40:     4.9449     4.8088   [    -10.422     28.788 ]\n",
      "                         :    var41:     7.9424     5.5420   [    -8.4661     31.055 ]\n",
      "                         :    var42:     11.034     5.7483   [    -6.7219     34.394 ]\n",
      "                         :    var43:     12.846     5.4598   [    -6.0367     35.064 ]\n",
      "                         :    var44:     12.885     5.5471   [    -5.3399     36.017 ]\n",
      "                         :    var45:     11.017     5.7452   [    -8.1370     34.427 ]\n",
      "                         :    var46:     7.9332     5.5564   [    -9.6217     31.459 ]\n",
      "                         :    var47:     4.9462     4.8152   [    -9.0932     24.864 ]\n",
      "                         :    var48:     3.7166     4.3715   [    -10.844     25.228 ]\n",
      "                         :    var49:     6.1139     5.1265   [    -8.5402     29.845 ]\n",
      "                         :    var50:     8.3390     5.4683   [    -8.2915     32.722 ]\n",
      "                         :    var51:     9.8672     5.5727   [    -8.3201     33.142 ]\n",
      "                         :    var52:     9.8495     5.5322   [    -8.3466     33.869 ]\n",
      "                         :    var53:     8.4185     5.4828   [    -9.7019     33.503 ]\n",
      "                         :    var54:     6.0872     5.1308   [    -9.1295     32.443 ]\n",
      "                         :    var55:     3.7002     4.3962   [    -10.585     27.256 ]\n",
      "                         :    var56:     2.4642     3.9476   [    -9.7419     22.620 ]\n",
      "                         :    var57:     4.0422     4.5454   [    -8.2481     26.215 ]\n",
      "                         :    var58:     5.5688     4.8957   [    -7.9015     30.791 ]\n",
      "                         :    var59:     6.5518     5.0494   [    -8.9191     29.350 ]\n",
      "                         :    var60:     6.5388     5.0481   [    -8.4943     28.476 ]\n",
      "                         :    var61:     5.6626     4.9085   [    -9.0860     29.674 ]\n",
      "                         :    var62:     4.0584     4.5142   [    -9.4862     23.124 ]\n",
      "                         :    var63:     2.5276     3.9035   [    -10.527     21.548 ]\n",
      "                         : -----------------------------------------------------------\n",
      "                         : \u001b[32m\n",
      "                         : <PlotVariables> Will not produce scatter plots ==> \n",
      "                         : |  The number of 64 input variables and 0 target values would require 2016 two-dimensional\n",
      "                         : |  histograms, which would occupy the computer's memory. Note that this\n",
      "                         : |  suppression does not have any consequences for your analysis, other\n",
      "                         : |  than not disposing of these scatter plots. You can modify the maximum\n",
      "                         : |  number of input variables allowed to generate scatter plots in your\n",
      "                         : |  script via the command line:\n",
      "                         : |  \"(TMVA::gConfig().GetVariablePlotting()).fMaxNumOfAllowedVariablesForScatterPlots = <some int>;\"\u001b[0m\n",
      "                         : \n",
      "                         : Some more output\n",
      "                         : \n",
      "                         : Evaluation results ranked by best signal efficiency and purity (area)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : DataSet       MVA                       \n",
      "                         : Name:         Method:          ROC-integ\n",
      "                         : dataset       BDT            : 0.787\n",
      "                         : dataset       DL_CNN         : 0.785\n",
      "                         : dataset       DL_DENSE       : 0.773\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : \n",
      "                         : Testing efficiency compared to training efficiency (overtraining check)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : DataSet              MVA              Signal efficiency: from test sample (from training sample) \n",
      "                         : Name:                Method:          @B=0.01             @B=0.10            @B=0.30   \n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : dataset              BDT            : 0.115 (0.195)       0.443 (0.536)      0.728 (0.781)\n",
      "                         : dataset              DL_CNN         : 0.097 (0.163)       0.420 (0.499)      0.723 (0.771)\n",
      "                         : dataset              DL_DENSE       : 0.098 (0.165)       0.403 (0.515)      0.711 (0.770)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:dataset          : Created tree 'TestTree' with 10000 events\n",
      "                         : \n",
      "Dataset:dataset          : Created tree 'TrainTree' with 10000 events\n",
      "                         : \n",
      "Factory                  : \u001b[1mThank you for using TMVA!\u001b[0m\n",
      "                         : \u001b[1mFor citation information, please visit: http://tmva.sf.net/citeTMVA.html\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "factory.EvaluateAllMethods();    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot ROC Curve\n",
    "We enable JavaScript visualisation for the plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "//%jsroot on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArgAAAHYCAIAAAApvgy/AAAABmJLR0QAAAAAAAD5Q7t/AAAgAElE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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "c1 = factory.GetROCCurve(loader);\n",
    "c1->Draw();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "// close outputfile to save output file\n",
    "outputFile->Close();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ROOT C++",
   "language": "c++",
   "name": "root"
  },
  "language_info": {
   "codemirror_mode": "text/x-c++src",
   "file_extension": ".C",
   "mimetype": " text/x-c++src",
   "name": "c++"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
